#!/usr/bin/env python3
"""
å®æ—¶èšåˆè„šæœ¬
ç›‘æ§WebSocketæ•°æ®å¹¶å®æ—¶ç”Ÿæˆ1åˆ†é’Ÿç‰¹å¾
"""

import asyncio
import pandas as pd
import numpy as np
from datetime import datetime, timezone, timedelta
from pathlib import Path
import logging
import glob
import os
import time
from typing import Dict, List, Optional
import json

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('realtime_aggregator.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class RealtimeAggregator:
    """å®æ—¶èšåˆå™¨"""
    
    def __init__(self, symbol: str = "ETH-USDT", output_dir: str = "data/realtime_features"):
        self.symbol = symbol
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ•°æ®ç¼“å†²åŒº
        self.orderbook_buffer = []
        self.current_minute = None
        self.last_aggregation_time = None
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "files_processed": 0,
            "rows_processed": 0,
            "features_generated": 0,
            "start_time": None
        }
    
    def load_latest_orderbook_files(self) -> List[str]:
        """åŠ è½½æœ€æ–°çš„OrderBookæ–‡ä»¶"""
        # æŸ¥æ‰¾ä»Šå¤©çš„OrderBookæ–‡ä»¶
        today = datetime.utcnow().strftime("%Y%m%d")
        pattern = f"data/websocket/{today}/orderbook_{self.symbol}_*.parquet"
        files = glob.glob(pattern)
        
        if not files:
            # å¦‚æœæ²¡æœ‰æŒ‰æ—¥æœŸåˆ†åŒºçš„æ–‡ä»¶ï¼ŒæŸ¥æ‰¾æ ¹ç›®å½•
            pattern = f"data/websocket/orderbook_{self.symbol}_*.parquet"
            files = glob.glob(pattern)
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œåªå¤„ç†æ–°æ–‡ä»¶
        files.sort(key=os.path.getmtime)
        
        if self.last_aggregation_time:
            # åªå¤„ç†ä¸Šæ¬¡èšåˆåçš„æ–°æ–‡ä»¶
            new_files = []
            for file in files:
                if os.path.getmtime(file) > self.last_aggregation_time:
                    new_files.append(file)
            files = new_files
        
        return files
    
    def process_orderbook_file(self, filepath: str) -> pd.DataFrame:
        """å¤„ç†å•ä¸ªOrderBookæ–‡ä»¶"""
        try:
            df = pd.read_parquet(filepath)
            logger.info(f"ğŸ“– å¤„ç†æ–‡ä»¶: {filepath} ({len(df)} è¡Œ)")
            return df
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {filepath}: {e}")
            return pd.DataFrame()
    
    def calculate_micro_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—å¾®è§‚ç‰¹å¾"""
        if df.empty:
            return df
        
        features = []
        
        for _, row in df.iterrows():
            # æå–ä»·æ ¼å’Œæ•°é‡
            bid_prices = []
            bid_sizes = []
            ask_prices = []
            ask_sizes = []
            
            for i in range(1, 6):
                if f'bid{i}_price' in row and f'bid{i}_size' in row:
                    bid_prices.append(row[f'bid{i}_price'])
                    bid_sizes.append(row[f'bid{i}_size'])
                if f'ask{i}_price' in row and f'ask{i}_size' in row:
                    ask_prices.append(row[f'ask{i}_price'])
                    ask_sizes.append(row[f'ask{i}_size'])
            
            if not bid_prices or not ask_prices:
                continue
            
            # è®¡ç®—åŸºç¡€ç‰¹å¾
            best_bid = bid_prices[0]
            best_ask = ask_prices[0]
            mid_price = (best_bid + best_ask) / 2
            spread = best_ask - best_bid
            spread_bps = spread / best_bid * 10000
            
            # è®¡ç®—VWAP
            total_bid_volume = sum(bid_sizes)
            total_ask_volume = sum(ask_sizes)
            
            if total_bid_volume > 0 and total_ask_volume > 0:
                bid_vwap = sum(p * s for p, s in zip(bid_prices, bid_sizes)) / total_bid_volume
                ask_vwap = sum(p * s for p, s in zip(ask_prices, ask_sizes)) / total_ask_volume
                vwap = (bid_vwap + ask_vwap) / 2
            else:
                vwap = mid_price
            
            # è®¡ç®—è®¢å•æµç‰¹å¾
            volume_imbalance = (total_bid_volume - total_ask_volume) / (total_bid_volume + total_ask_volume)
            price_pressure = (ask_vwap - bid_vwap) / mid_price
            
            # è®¡ç®—æµåŠ¨æ€§ç‰¹å¾
            liquidity_score = (total_bid_volume + total_ask_volume) / spread_bps if spread_bps > 0 else 0
            
            feature_row = {
                'timestamp': row['timestamp'],
                'mid_price': mid_price,
                'bid_price': best_bid,
                'ask_price': best_ask,
                'spread': spread,
                'spread_bps': spread_bps,
                'vwap': vwap,
                'bid_vwap': bid_vwap,
                'ask_vwap': ask_vwap,
                'total_bid_volume': total_bid_volume,
                'total_ask_volume': total_ask_volume,
                'volume_imbalance': volume_imbalance,
                'price_pressure': price_pressure,
                'liquidity_score': liquidity_score,
                'price_mean': np.mean(bid_prices + ask_prices),
                'price_std': np.std(bid_prices + ask_prices),
                'price_range': max(ask_prices) - min(bid_prices)
            }
            
            features.append(feature_row)
        
        return pd.DataFrame(features)
    
    def aggregate_to_minute(self, df: pd.DataFrame) -> pd.DataFrame:
        """èšåˆåˆ°1åˆ†é’Ÿ"""
        if df.empty:
            return df
        
        # ç¡®ä¿æ—¶é—´æˆ³æ ¼å¼
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        
        # è®¾ç½®æ—¶é—´ç´¢å¼•
        df = df.set_index('timestamp').sort_index()
        
        # 1åˆ†é’Ÿèšåˆè§„åˆ™
        agg_rules = {
            'mid_price': 'last',
            'bid_price': 'last',
            'ask_price': 'last',
            'spread': 'mean',
            'spread_bps': 'mean',
            'vwap': 'mean',
            'bid_vwap': 'mean',
            'ask_vwap': 'mean',
            'total_bid_volume': 'sum',
            'total_ask_volume': 'sum',
            'volume_imbalance': 'mean',
            'price_pressure': 'mean',
            'liquidity_score': 'mean',
            'price_mean': 'mean',
            'price_std': 'mean',
            'price_range': 'max'
        }
        
        # æ‰§è¡Œèšåˆ
        resampled = df.resample('1T').agg(agg_rules)
        
        # è®¡ç®—é¢å¤–ç‰¹å¾
        resampled = self.calculate_minute_features(resampled)
        
        return resampled.reset_index()
    
    def calculate_minute_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—1åˆ†é’Ÿé¢å¤–ç‰¹å¾"""
        # ä»·æ ¼åŠ¨é‡
        df['price_momentum_1m'] = df['mid_price'].pct_change(1)
        df['price_momentum_3m'] = df['mid_price'].pct_change(3)
        df['price_momentum_5m'] = df['mid_price'].pct_change(5)
        
        # æˆäº¤é‡ç‰¹å¾
        df['total_volume'] = df['total_bid_volume'] + df['total_ask_volume']
        df['volume_ma_3m'] = df['total_volume'].rolling(3).mean()
        df['volume_ratio'] = df['total_volume'] / df['volume_ma_3m']
        
        # ä»·å·®ç‰¹å¾
        df['spread_ma'] = df['spread_bps'].rolling(5).mean()
        df['spread_ratio'] = df['spread_bps'] / df['spread_ma']
        
        # è®¢å•æµç‰¹å¾
        df['imbalance_ma'] = df['volume_imbalance'].rolling(5).mean()
        df['imbalance_trend'] = df['imbalance_ma'].pct_change()
        
        # æµåŠ¨æ€§ç‰¹å¾
        df['liquidity_ma'] = df['liquidity_score'].rolling(5).mean()
        df['liquidity_trend'] = df['liquidity_ma'].pct_change()
        
        # æ³¢åŠ¨ç‡ç‰¹å¾
        df['price_volatility'] = df['price_momentum_1m'].rolling(5).std()
        df['volatility_ma'] = df['price_volatility'].rolling(5).mean()
        df['volatility_ratio'] = df['price_volatility'] / df['volatility_ma']
        
        # å¼‚å¸¸æ£€æµ‹
        df['price_jump'] = (abs(df['price_momentum_1m']) > df['price_momentum_1m'].rolling(20).quantile(0.95)).astype(int)
        df['volume_spike'] = (df['volume_ratio'] > 2.0).astype(int)
        df['spread_widening'] = (df['spread_ratio'] > 1.5).astype(int)
        
        return df
    
    def save_features(self, df: pd.DataFrame):
        """ä¿å­˜ç‰¹å¾æ•°æ®"""
        if df.empty:
            return
        
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"realtime_features_{self.symbol}_{timestamp}.parquet"
            filepath = self.output_dir / filename
            
            # ä½¿ç”¨zstdå‹ç¼©
            df.to_parquet(filepath, index=False, compression='zstd')
            
            file_size_mb = filepath.stat().st_size / (1024 * 1024)
            self.stats['features_generated'] += 1
            
            logger.info(f"ğŸ’¾ ä¿å­˜ç‰¹å¾: {len(df)} è¡Œ -> {filepath} ({file_size_mb:.2f}MB)")
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç‰¹å¾å¤±è´¥: {e}")
    
    async def run_aggregation(self):
        """è¿è¡Œèšåˆä»»åŠ¡"""
        logger.info("ğŸš€ å¯åŠ¨å®æ—¶èšåˆå™¨...")
        self.stats['start_time'] = datetime.now()
        
        while True:
            try:
                # åŠ è½½æ–°æ–‡ä»¶
                files = self.load_latest_orderbook_files()
                
                if files:
                    logger.info(f"ğŸ“ å‘ç° {len(files)} ä¸ªæ–°æ–‡ä»¶")
                    
                    all_data = []
                    for file in files:
                        df = self.process_orderbook_file(file)
                        if not df.empty:
                            # è®¡ç®—å¾®è§‚ç‰¹å¾
                            micro_features = self.calculate_micro_features(df)
                            all_data.append(micro_features)
                            self.stats['rows_processed'] += len(df)
                    
                    if all_data:
                        # åˆå¹¶æ‰€æœ‰æ•°æ®
                        combined_df = pd.concat(all_data, ignore_index=True)
                        
                        # èšåˆåˆ°1åˆ†é’Ÿ
                        minute_features = self.aggregate_to_minute(combined_df)
                        
                        if not minute_features.empty:
                            # ä¿å­˜ç‰¹å¾
                            self.save_features(minute_features)
                            self.stats['files_processed'] += len(files)
                
                # æ›´æ–°æœ€åå¤„ç†æ—¶é—´
                self.last_aggregation_time = time.time()
                
                # æ‰“å°ç»Ÿè®¡
                elapsed = datetime.now() - self.stats['start_time']
                logger.info(f"ğŸ“Š ç»Ÿè®¡: å¤„ç†{self.stats['files_processed']}æ–‡ä»¶, "
                          f"{self.stats['rows_processed']}è¡Œ, "
                          f"ç”Ÿæˆ{self.stats['features_generated']}ç‰¹å¾æ–‡ä»¶ "
                          f"({elapsed.total_seconds():.0f}s)")
                
                # ç­‰å¾…30ç§’åå†æ¬¡æ£€æŸ¥
                await asyncio.sleep(30)
                
            except Exception as e:
                logger.error(f"âŒ èšåˆä»»åŠ¡å¼‚å¸¸: {e}")
                await asyncio.sleep(30)

async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¯åŠ¨å®æ—¶èšåˆå™¨")
    
    aggregator = RealtimeAggregator(symbol="ETH-USDT")
    await aggregator.run_aggregation()

if __name__ == "__main__":
    asyncio.run(main()) 