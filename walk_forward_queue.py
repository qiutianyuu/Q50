#!/usr/bin/env python3
"""
æ»šåŠ¨Walk-Forwardè®­ç»ƒè„šæœ¬ï¼ˆé›†æˆé˜Ÿåˆ—ç‰¹å¾ï¼‰
ä½¿ç”¨é˜Ÿåˆ—æ¨¡æ‹Ÿå™¨ç‰¹å¾è¿›è¡Œæ»šåŠ¨è®­ç»ƒå’ŒéªŒè¯
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import xgboost as xgb
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
import json
import glob
import os
from utils.labeling import make_labels, get_label_stats

class WalkForwardQueue:
    def __init__(self, train_hours: int = 3, test_hours: int = 0.5, step_hours: int = 0.5):
        """
        åˆå§‹åŒ–Walk-Forwardè®­ç»ƒå™¨
        
        Args:
            train_hours: è®­ç»ƒçª—å£å°æ—¶æ•°
            test_hours: æµ‹è¯•çª—å£å°æ—¶æ•°
            step_hours: æ»šåŠ¨æ­¥é•¿å°æ—¶æ•°
        """
        self.train_hours = train_hours
        self.test_hours = test_hours
        self.step_hours = step_hours
        
    def load_latest_queue_features(self):
        """åŠ è½½æœ€æ–°çš„é˜Ÿåˆ—ç‰¹å¾æ•°æ®"""
        files = glob.glob("data/realtime_features_queue_*.parquet")
        if not files:
            raise FileNotFoundError("No queue features files found")
        latest_file = max(files, key=os.path.getctime)
        print(f"Loading: {latest_file}")
        df = pd.read_parquet(latest_file)
        print(f"Loaded {len(df)} rows")
        return df, latest_file
    
    def create_walk_forward_splits(self, df: pd.DataFrame) -> list:
        """åˆ›å»ºWalk-Forwardåˆ†å‰²"""
        df = df.sort_values('timestamp').reset_index(drop=True)
        
        # è½¬æ¢ä¸ºå°æ—¶
        train_minutes = int(self.train_hours * 60)
        test_minutes = int(self.test_hours * 60)
        step_minutes = int(self.step_hours * 60)
        
        # ä¼°ç®—æ¯åˆ†é’Ÿçš„è¡Œæ•°
        total_minutes = (df['timestamp'].max() - df['timestamp'].min()).total_seconds() / 60
        rows_per_minute = len(df) / total_minutes
        
        train_rows = int(train_minutes * rows_per_minute)
        test_rows = int(test_minutes * rows_per_minute)
        step_rows = int(step_minutes * rows_per_minute)
        
        print(f"ä¼°ç®—å‚æ•°: è®­ç»ƒ{train_rows}è¡Œ, æµ‹è¯•{test_rows}è¡Œ, æ­¥é•¿{step_rows}è¡Œ")
        
        splits = []
        start_idx = 0
        
        while start_idx + train_rows + test_rows <= len(df):
            train_end = start_idx + train_rows
            test_end = train_end + test_rows
            
            split = {
                'split_id': len(splits),
                'train_start': start_idx,
                'train_end': train_end,
                'test_start': train_end,
                'test_end': test_end,
                'train_data': df.iloc[start_idx:train_end],
                'test_data': df.iloc[train_end:test_end]
            }
            
            splits.append(split)
            start_idx += step_rows
        
        print(f"åˆ›å»ºäº† {len(splits)} ä¸ªWalk-Forwardåˆ†å‰²")
        return splits
    
    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """å‡†å¤‡ç‰¹å¾åˆ—"""
        # æ’é™¤éç‰¹å¾åˆ—
        exclude_cols = ['timestamp']
        feature_cols = [col for col in df.columns if col not in exclude_cols]
        return df[feature_cols]
    
    def generate_labels(self, df: pd.DataFrame, horizon: int = 240, alpha: float = 1.0, 
                       mode: str = 'maker', require_fill: bool = True) -> pd.Series:
        """ç”Ÿæˆæ ‡ç­¾"""
        labels = make_labels(df['mid_price'], df['rel_spread'], horizon, alpha, mode=mode, require_fill=require_fill)
        return labels
    
    def train_and_evaluate(self, train_df: pd.DataFrame, test_df: pd.DataFrame, 
                          label_name: str) -> dict:
        """è®­ç»ƒå’Œè¯„ä¼°æ¨¡å‹"""
        print(f"\n=== è®­ç»ƒæ¨¡å‹: {label_name} ===")
        
        # å‡†å¤‡ç‰¹å¾
        X_train = self.prepare_features(train_df)
        X_test = self.prepare_features(test_df)
        
        # ç”Ÿæˆæ ‡ç­¾
        y_train = self.generate_labels(train_df)
        y_test = self.generate_labels(test_df)
        
        # ç§»é™¤ä¸­æ€§æ ‡ç­¾
        train_mask = y_train != 0
        test_mask = y_test != 0
        
        X_train_filtered = X_train[train_mask]
        y_train_filtered = y_train[train_mask]
        X_test_filtered = X_test[test_mask]
        y_test_filtered = y_test[test_mask]
        
        if len(X_train_filtered) == 0 or len(X_test_filtered) == 0:
            print("æ²¡æœ‰æœ‰æ•ˆæ ‡ç­¾ï¼Œè·³è¿‡è®­ç»ƒ")
            return None
        
        # è½¬æ¢ä¸ºäºŒåˆ†ç±»
        y_train_binary = (y_train_filtered == 1).astype(int)
        y_test_binary = (y_test_filtered == 1).astype(int)
        
        print(f"è®­ç»ƒæ ·æœ¬: {len(X_train_filtered)} (long: {y_train_binary.sum()}, short: {len(y_train_binary)-y_train_binary.sum()})")
        print(f"æµ‹è¯•æ ·æœ¬: {len(X_test_filtered)} (long: {y_test_binary.sum()}, short: {len(y_test_binary)-y_test_binary.sum()})")
        
        # è®­ç»ƒæ¨¡å‹
        model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            eval_metric='auc'
        )
        
        model.fit(X_train_filtered, y_train_binary)
        
        # è¯„ä¼°
        y_pred = model.predict(X_test_filtered)
        y_proba = model.predict_proba(X_test_filtered)[:, 1]
        
        train_accuracy = accuracy_score(y_train_binary, model.predict(X_train_filtered))
        test_accuracy = accuracy_score(y_test_binary, y_pred)
        train_auc = roc_auc_score(y_train_binary, model.predict_proba(X_train_filtered)[:, 1])
        test_auc = roc_auc_score(y_test_binary, y_proba)
        
        # ç‰¹å¾é‡è¦æ€§
        feature_importance = pd.DataFrame({
            'feature': X_train.columns,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        results = {
            'train_accuracy': train_accuracy,
            'test_accuracy': test_accuracy,
            'train_auc': train_auc,
            'test_auc': test_auc,
            'feature_importance': feature_importance.head(10).to_dict('records'),
            'train_samples': len(X_train_filtered),
            'test_samples': len(X_test_filtered),
            'model': model
        }
        
        print(f"è®­ç»ƒå‡†ç¡®ç‡: {train_accuracy:.3f}, æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.3f}")
        print(f"è®­ç»ƒAUC: {train_auc:.3f}, æµ‹è¯•AUC: {test_auc:.3f}")
        
        return results
    
    def run_walk_forward(self, label_params: list = None) -> dict:
        """è¿è¡Œå®Œæ•´çš„Walk-Forwardè®­ç»ƒ"""
        print("ğŸš€ å¼€å§‹Walk-Forwardè®­ç»ƒï¼ˆé›†æˆé˜Ÿåˆ—ç‰¹å¾ï¼‰...")
        
        # åŠ è½½æ•°æ®
        df, input_file = self.load_latest_queue_features()
        
        # é»˜è®¤æ ‡ç­¾å‚æ•°
        if label_params is None:
            label_params = [
                {'horizon': 240, 'alpha': 1.0, 'mode': 'maker', 'require_fill': True, 'name': 'h240_a1.0_maker_fill'},
                {'horizon': 240, 'alpha': 0.6, 'mode': 'maker', 'require_fill': True, 'name': 'h240_a0.6_maker_fill'},
                {'horizon': 60, 'alpha': 0.3, 'mode': 'maker', 'require_fill': True, 'name': 'h60_a0.3_maker_fill'}
            ]
        
        # åˆ›å»ºåˆ†å‰²
        splits = self.create_walk_forward_splits(df)
        
        all_results = {}
        
        for label_param in label_params:
            print(f"\n{'='*60}")
            print(f"å¤„ç†æ ‡ç­¾: {label_param['name']}")
            print(f"{'='*60}")
            
            split_results = []
            
            for split in splits:
                print(f"\n--- Split {split['split_id']} ---")
                
                # è®­ç»ƒå’Œè¯„ä¼°
                result = self.train_and_evaluate(
                    split['train_data'], 
                    split['test_data'], 
                    label_param['name']
                )
                
                if result is not None:
                    result['split_id'] = split['split_id']
                    result['label_params'] = label_param
                    split_results.append(result)
            
            # æ±‡æ€»ç»“æœ
            if split_results:
                all_results[label_param['name']] = {
                    'split_results': split_results,
                    'summary': self.summarize_results(split_results)
                }
        
        return all_results
    
    def summarize_results(self, split_results: list) -> dict:
        """æ±‡æ€»åˆ†å‰²ç»“æœ"""
        train_accuracies = [r['train_accuracy'] for r in split_results]
        test_accuracies = [r['test_accuracy'] for r in split_results]
        train_aucs = [r['train_auc'] for r in split_results]
        test_aucs = [r['test_auc'] for r in split_results]
        
        summary = {
            'num_splits': len(split_results),
            'avg_train_accuracy': np.mean(train_accuracies),
            'avg_test_accuracy': np.mean(test_accuracies),
            'avg_train_auc': np.mean(train_aucs),
            'avg_test_auc': np.mean(test_aucs),
            'std_train_accuracy': np.std(train_accuracies),
            'std_test_accuracy': np.std(test_accuracies),
            'std_train_auc': np.std(train_aucs),
            'std_test_auc': np.std(test_aucs),
            'overfitting_score': np.mean(train_aucs) - np.mean(test_aucs)
        }
        
        return summary
    
    def save_results(self, results: dict, output_file: str = None):
        """ä¿å­˜ç»“æœ"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"walk_forward_results_queue_{timestamp}.json"
        
        # è½¬æ¢æ¨¡å‹å¯¹è±¡ä¸ºå¯åºåˆ—åŒ–æ ¼å¼
        serializable_results = {}
        for label_name, label_results in results.items():
            serializable_results[label_name] = {
                'summary': label_results['summary'],
                'split_results': []
            }
            
            for split_result in label_results['split_results']:
                serializable_split = {k: v for k, v in split_result.items() if k != 'model'}
                serializable_results[label_name]['split_results'].append(serializable_split)
        
        with open(output_file, 'w') as f:
            json.dump(serializable_results, f, indent=2, default=str)
        
        print(f"ç»“æœå·²ä¿å­˜: {output_file}")
        return output_file

def main():
    # åˆ›å»ºWalk-Forwardè®­ç»ƒå™¨ - ä½¿ç”¨æ›´å°çš„çª—å£
    wf = WalkForwardQueue(train_hours=0.5, test_hours=0.1, step_hours=0.1)
    
    # è¿è¡Œè®­ç»ƒ
    results = wf.run_walk_forward()
    
    # ä¿å­˜ç»“æœ
    wf.save_results(results)
    
    # æ‰“å°æ±‡æ€»
    print("\n" + "="*80)
    print("WALK-FORWARD è®­ç»ƒæ±‡æ€»")
    print("="*80)
    
    for label_name, label_results in results.items():
        summary = label_results['summary']
        print(f"\nğŸ“Š {label_name}:")
        print(f"  åˆ†å‰²æ•°: {summary['num_splits']}")
        print(f"  å¹³å‡è®­ç»ƒAUC: {summary['avg_train_auc']:.3f} Â± {summary['std_train_auc']:.3f}")
        print(f"  å¹³å‡æµ‹è¯•AUC: {summary['avg_test_auc']:.3f} Â± {summary['std_test_auc']:.3f}")
        print(f"  è¿‡æ‹Ÿåˆåˆ†æ•°: {summary['overfitting_score']:.3f}")
        print(f"  å¹³å‡è®­ç»ƒå‡†ç¡®ç‡: {summary['avg_train_accuracy']:.3f}")
        print(f"  å¹³å‡æµ‹è¯•å‡†ç¡®ç‡: {summary['avg_test_accuracy']:.3f}")
    
    return results

if __name__ == "__main__":
    main() 