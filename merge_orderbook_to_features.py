#!/usr/bin/env python3
"""
ç¦»çº¿æ‰¹é‡æŠŠæ‰€æœ‰ orderbook & trades æ•°æ®è½¬æ¢æˆå¾®è§‚ç‰¹å¾
æŒ‰æ–‡ä»¶æµå¼è¯»å–â†’è®¡ç®—â†’ç›´æ¥å†™ Parquetï¼Œé¿å…ä¸€æ¬¡æ€§å æ»¡å†…å­˜
"""

import glob
import os
import pandas as pd
import numpy as np
from datetime import datetime
import pyarrow as pa
import pyarrow.parquet as pq
from realtime_feature_engineering import RealtimeFeatureEngineering
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def calculate_micro_price_features_batch(orderbook_df: pd.DataFrame) -> pd.DataFrame:
    """æ‰¹é‡è®¡ç®—å¾®ä»·æ ¼ç‰¹å¾"""
    if orderbook_df.empty:
        return pd.DataFrame()
    
    features = []
    
    for _, row in orderbook_df.iterrows():
        # ç›´æ¥ä½¿ç”¨è§£æå¥½çš„OrderBookæ•°æ®
        bid_prices = []
        bid_sizes = []
        ask_prices = []
        ask_sizes = []
        
        # æå–5æ¡£ä¹°å–ç›˜æ•°æ®
        for i in range(1, 6):
            bid_price_col = f'bid{i}_price'
            bid_size_col = f'bid{i}_size'
            ask_price_col = f'ask{i}_price'
            ask_size_col = f'ask{i}_size'
            
            if bid_price_col in row and bid_size_col in row:
                bid_prices.append(row[bid_price_col])
                bid_sizes.append(row[bid_size_col])
            
            if ask_price_col in row and ask_size_col in row:
                ask_prices.append(row[ask_price_col])
                ask_sizes.append(row[ask_size_col])
        
        if not bid_prices or not ask_prices:
            continue
        
        # åŠ æƒå¹³å‡ä»·æ ¼ (VWAP)
        bid_vwap = np.average(bid_prices, weights=bid_sizes) if bid_sizes else 0
        ask_vwap = np.average(ask_prices, weights=ask_sizes) if ask_sizes else 0
        
        # ä¸­é—´ä»·
        mid_price = (bid_prices[0] + ask_prices[0]) / 2 if bid_prices and ask_prices else 0
        
        # ä¹°å–ä»·å·®
        spread = ask_prices[0] - bid_prices[0] if bid_prices and ask_prices else 0
        spread_bps = (spread / mid_price * 10000) if mid_price > 0 else 0
        rel_spread = (spread / mid_price) if mid_price > 0 else 0
        
        # è®¢å•ç°¿ä¸å¹³è¡¡
        bid_volume = sum(bid_sizes)
        ask_volume = sum(ask_sizes)
        volume_imbalance = (bid_volume - ask_volume) / (bid_volume + ask_volume) if (bid_volume + ask_volume) > 0 else 0
        
        # ä»·æ ¼å‹åŠ›
        price_pressure = (ask_vwap - mid_price) / mid_price if mid_price > 0 else 0
        
        # å¡«å……æ¦‚ç‡å’Œä»·æ ¼å½±å“ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
        bid_fill_prob = 0.5  # é»˜è®¤å€¼
        ask_fill_prob = 0.5
        bid_price_impact = 0.0
        ask_price_impact = 0.0
        
        # é˜Ÿåˆ—æ·±åº¦
        bid_queue_depth = bid_volume
        ask_queue_depth = ask_volume
        
        # å¯ç”¨æ¡£ä½
        bid_levels_available = len(bid_prices)
        ask_levels_available = len(ask_prices)
        
        # æœ€ä¼˜æŒ‚å•å¤§å°ï¼ˆç®€åŒ–ï¼‰
        optimal_bid_size = bid_sizes[0] if bid_sizes else 0
        optimal_ask_size = ask_sizes[0] if ask_sizes else 0
        
        features.append({
            'timestamp': row.get('timestamp', pd.Timestamp.now()),
            'bid_price': bid_prices[0] if bid_prices else 0,
            'ask_price': ask_prices[0] if ask_prices else 0,
            'mid_price': mid_price,
            'rel_spread': rel_spread,
            'bid_vwap': bid_vwap,
            'ask_vwap': ask_vwap,
            'spread': spread,
            'spread_bps': spread_bps,
            'volume_imbalance': volume_imbalance,
            'price_pressure': price_pressure,
            'bid_volume': bid_volume,
            'ask_volume': ask_volume,
            'total_volume_x': bid_volume + ask_volume,
            'price_mean': mid_price,
            'price_std': 0.0,
            'price_range': 0.0,
            'total_volume_y': bid_volume + ask_volume,
            'avg_trade_size': 0.0,
            'large_trades': 0,
            'buy_ratio': 0.5,
            'price_momentum': 0.0,
            'vwap': mid_price,
            'trade_frequency': 0.0,
            'trade_count': 0,
            'bid_fill_prob': bid_fill_prob,
            'ask_fill_prob': ask_fill_prob,
            'bid_price_impact': bid_price_impact,
            'ask_price_impact': ask_price_impact,
            'optimal_bid_size': optimal_bid_size,
            'optimal_ask_size': optimal_ask_size,
            'bid_queue_depth': bid_queue_depth,
            'ask_queue_depth': ask_queue_depth,
            'bid_levels_available': bid_levels_available,
            'ask_levels_available': ask_levels_available,
            'price_change': 0.0,
            'price_change_abs': 0.0,
            'buy_pressure_change': 0.0,
            'spread_change': 0.0,
            'price_volatility': 0.0,
            'price_trend': 0.0,
            'trend_deviation': 0.0,
            'bid_fill_prob_change': 0.0,
            'ask_fill_prob_change': 0.0,
            'fill_prob_imbalance': 0.0,
            'price_impact_imbalance': 0.0
        })
    
    return pd.DataFrame(features)

def flush_batches(batches, symbol, batch_num):
    """å°†æ‰¹æ¬¡æ•°æ®å†™å…¥æ–‡ä»¶"""
    if not batches:
        return
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    outfile = f"data/micro_features_{symbol}_{timestamp}_batch_{batch_num}.parquet"
    
    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
    combined_df = pd.concat(batches, ignore_index=True)
    combined_df.to_parquet(outfile, compression="zstd", index=False)
    
    total_rows = sum(len(df) for df in batches)
    logger.info(f"â–¶ æ‰¹æ¬¡ {batch_num} å·²å†™å…¥: {outfile} ({total_rows} è¡Œ)")
    
    return outfile

def main(symbol="ETH-USDT"):
    """ä¸»å‡½æ•°ï¼šæ‰¹é‡å¤„ç†æ‰€æœ‰orderbookæ–‡ä»¶"""
    logger.info(f"å¼€å§‹æ‰¹é‡å¤„ç† {symbol} çš„orderbookæ•°æ®...")
    
    # è·å–æ‰€æœ‰orderbookæ–‡ä»¶
    order_files = sorted(glob.glob(f"data/websocket/orderbook_{symbol}_*.parquet"))
    logger.info(f"æ‰¾åˆ° {len(order_files)} ä¸ªorderbookæ–‡ä»¶")
    
    if not order_files:
        logger.error("æ²¡æœ‰æ‰¾åˆ°orderbookæ–‡ä»¶")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("data", exist_ok=True)
    
    all_batches = []  # å½“å‰æ‰¹æ¬¡çš„æ•°æ®æ¡†åˆ—è¡¨
    batch_num = 1
    total_processed = 0
    output_files = []
    
    for idx, file_path in enumerate(order_files, 1):
        try:
            # è¯»å–orderbookæ–‡ä»¶
            logger.info(f"å¤„ç†æ–‡ä»¶ {idx}/{len(order_files)}: {os.path.basename(file_path)}")
            orderbook_df = pd.read_parquet(file_path)
            
            # ç¡®ä¿æ—¶é—´æˆ³åˆ—å­˜åœ¨
            if 'ts' in orderbook_df.columns:
                orderbook_df['timestamp'] = pd.to_datetime(orderbook_df['ts'], unit='ms')
            elif 'timestamp' not in orderbook_df.columns:
                orderbook_df['timestamp'] = pd.to_datetime('now')
            
            # è®¡ç®—å¾®ä»·æ ¼ç‰¹å¾
            features_df = calculate_micro_price_features_batch(orderbook_df)
            
            if not features_df.empty:
                all_batches.append(features_df)
                total_processed += len(features_df)
            
            # æ¯ç´¯ç§¯ä¸€å®šè¡Œæ•°å°±å†™å…¥æ–‡ä»¶ï¼Œé¿å…å†…å­˜å ç”¨è¿‡å¤§
            current_batch_size = sum(len(df) for df in all_batches)
            if current_batch_size > 50000:  # æ¯5ä¸‡è¡Œå†™å…¥ä¸€æ¬¡
                outfile = flush_batches(all_batches, symbol, batch_num)
                if outfile:
                    output_files.append(outfile)
                all_batches = []
                batch_num += 1
            
            # è¿›åº¦æŠ¥å‘Š
            if idx % 50 == 0:
                logger.info(f"å·²å¤„ç† {idx}/{len(order_files)} æ–‡ä»¶ï¼Œç´¯è®¡ {total_processed} è¡Œç‰¹å¾")
                
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
            continue
    
    # å¤„ç†æœ€åä¸€æ‰¹
    if all_batches:
        outfile = flush_batches(all_batches, symbol, batch_num)
        if outfile:
            output_files.append(outfile)
    
    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶
    if len(output_files) > 1:
        logger.info("åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶...")
        combined_dfs = []
        for file_path in output_files:
            df = pd.read_parquet(file_path)
            combined_dfs.append(df)
        
        final_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        final_file = f"data/micro_features_{symbol}_{final_timestamp}.parquet"
        final_df = pd.concat(combined_dfs, ignore_index=True)
        final_df.to_parquet(final_file, compression="zstd", index=False)
        
        logger.info(f"âœ… æœ€ç»ˆåˆå¹¶æ–‡ä»¶: {final_file} ({len(final_df)} è¡Œ)")
        
        # æ¸…ç†ä¸´æ—¶æ‰¹æ¬¡æ–‡ä»¶
        for file_path in output_files:
            try:
                os.remove(file_path)
                logger.info(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {file_path}")
            except:
                pass
    elif len(output_files) == 1:
        # åªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼Œé‡å‘½å
        final_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        final_file = f"data/micro_features_{symbol}_{final_timestamp}.parquet"
        os.rename(output_files[0], final_file)
        logger.info(f"âœ… æœ€ç»ˆæ–‡ä»¶: {final_file}")
    
    logger.info(f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼æ€»å…±å¤„ç† {total_processed} è¡Œç‰¹å¾æ•°æ®")

if __name__ == "__main__":
    main() 