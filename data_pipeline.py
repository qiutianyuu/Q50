import pandas as pd
import numpy as np
import os
import requests
import time
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

class DataPipeline:
    def __init__(self):
        self.base_path = "/Users/qiutianyu"
        self.processed_path = f"{self.base_path}/data/processed"
        self.w1_path = f"{self.base_path}/ETHUSDT-w1"
        
        # APIé…ç½®
        self.dune_api_key = "sim_xZvnjKWCFpvVMhPAKK4idopF19hShv3f"
        self.etherscan_api_key = "CM56ZD9KTV8K93U8EXP8P4E1CBIEJ1P5"
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        os.makedirs(self.processed_path, exist_ok=True)
        os.makedirs(self.w1_path, exist_ok=True)
        
    def merge_kline_data(self, timeframe):
        """åˆå¹¶æŒ‡å®šæ—¶é—´å‘¨æœŸçš„Kçº¿æ•°æ®"""
        print(f"ğŸ”„ åˆå¹¶ {timeframe} æ•°æ®...")
        import re
        if timeframe == '5m':
            base_path = f"{self.base_path}/ETHUSDT-5m"
        else:
            base_path = f"{self.base_path}/ETHUSDT-{timeframe}"
        
        if not os.path.exists(base_path):
            print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {base_path}")
            return None
            
        all_data = []
        
        for item in os.listdir(base_path):
            item_path = os.path.join(base_path, item)
            if os.path.isdir(item_path) and item.startswith(f'ETHUSDT-{timeframe}-'):
                csv_files = [f for f in os.listdir(item_path) if f.endswith('.csv')]
                for csv_file in csv_files:
                    file_path = os.path.join(item_path, csv_file)
                    try:
                        # æ‰€æœ‰å‘¨æœŸéƒ½ç”¨æ­£åˆ™æå–13ä½æ¯«ç§’æ—¶é—´æˆ³
                        df = pd.read_csv(file_path, header=None, dtype={0: str}, usecols=range(12))
                        df[0] = df[0].str.extract(r'(\d{13})')[0]
                        df = df.dropna(subset=[0])
                        df[0] = df[0].astype('int64')
                        df = df.iloc[:, :6]
                        df.columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
                        all_data.append(df)
                        print(f"  âœ… åŠ è½½: {csv_file} ({len(df)} è¡Œ)")
                    except Exception as e:
                        print(f"  âŒ é”™è¯¯: {csv_file} - {e}")
        
        if not all_data:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ° {timeframe} æ•°æ®")
            return None
            
        merged_df = pd.concat(all_data, ignore_index=True)
        merged_df = merged_df.drop_duplicates(subset=['timestamp'])
        merged_df = merged_df.sort_values('timestamp').reset_index(drop=True)
        print(f"âœ… {timeframe} æ•°æ®åˆå¹¶å®Œæˆ: {len(merged_df)} è¡Œ")
        return merged_df
    
    def process_kline_data(self, df, timeframe):
        """å¤„ç†Kçº¿æ•°æ®ï¼šUTCå¯¹é½ã€æ·»åŠ æ—¶é—´æ ‡ç­¾"""
        if df is None or len(df) == 0:
            return None
            
        print(f"ğŸ”„ å¤„ç† {timeframe} æ•°æ®...")
        df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']].copy()
        print(f"  - åŸå§‹æ—¶é—´æˆ³ç±»å‹: {df['timestamp'].dtype}")
        print(f"  - æ—¶é—´æˆ³ç¤ºä¾‹: {df['timestamp'].iloc[0]}")
        if np.issubdtype(df['timestamp'].dtype, np.number):
            df['timestamp'] = pd.to_datetime(df['timestamp'].astype(int), unit='ms', utc=True)
        else:
            df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)
        # æ–­è¨€æ—¶é—´æˆ³èŒƒå›´
        min_ts = df['timestamp'].min()
        max_ts = df['timestamp'].max()
        print(f"  - å¤„ç†åæ—¶é—´èŒƒå›´: {min_ts} ~ {max_ts}")
        assert pd.Timestamp('2017-01-01', tz='UTC') <= min_ts <= pd.Timestamp('2030-01-01', tz='UTC'), 'æ—¶é—´æˆ³å¼‚å¸¸: min'
        assert pd.Timestamp('2017-01-01', tz='UTC') <= max_ts <= pd.Timestamp('2030-01-01', tz='UTC'), 'æ—¶é—´æˆ³å¼‚å¸¸: max'
        df['date'] = df['timestamp'].dt.date
        df['hour'] = df['timestamp'].dt.hour
        df['weekday'] = df['timestamp'].dt.weekday
        df['month'] = df['timestamp'].dt.month
        df['year'] = df['timestamp'].dt.year
        df['timeframe'] = timeframe
        df['returns'] = df['close'].pct_change()
        df['log_returns'] = np.log(df['close'] / df['close'].shift(1))
        df['ma_20'] = df['close'].rolling(window=20).mean()
        df['ma_50'] = df['close'].rolling(window=50).mean()
        df['volume_ma_20'] = df['volume'].rolling(window=20).mean()
        df['volume_pct'] = df['volume'] / df['volume_ma_20']
        print(f"âœ… {timeframe} æ•°æ®å¤„ç†å®Œæˆ")
        return df
    
    def fetch_dune_activity(self, address, limit=100):
        """è·å–Dune Activityæ•°æ®"""
        url = f"https://api.sim.dune.com/v1/evm/activity/{address}"
        headers = {"X-Sim-Api-Key": self.dune_api_key}
        
        try:
            response = requests.get(url, headers=headers, params={"limit": limit})
            if response.status_code == 200:
                return response.json()
            else:
                print(f"âŒ Dune APIé”™è¯¯: {response.status_code}")
                return None
        except Exception as e:
            print(f"âŒ Dune APIè¯·æ±‚å¤±è´¥: {e}")
            return None
    
    def fetch_dune_transactions(self, address, limit=100):
        """è·å–Dune Transactionsæ•°æ®"""
        url = f"https://api.sim.dune.com/v1/evm/transactions/{address}"
        headers = {"X-Sim-Api-Key": self.dune_api_key}
        
        try:
            response = requests.get(url, headers=headers, params={"limit": limit})
            if response.status_code == 200:
                return response.json()
            else:
                print(f"âŒ Dune Transactions APIé”™è¯¯: {response.status_code}")
                return None
        except Exception as e:
            print(f"âŒ Dune Transactions APIè¯·æ±‚å¤±è´¥: {e}")
            return None
    
    def fetch_etherscan_balance(self, address, chain_id):
        """è·å–Etherscanä½™é¢æ•°æ®"""
        url = "https://api.etherscan.io/v2/api"
        params = {
            "chainid": chain_id,
            "module": "account",
            "action": "balance",
            "address": address,
            "tag": "latest",
            "apikey": self.etherscan_api_key
        }
        
        try:
            response = requests.get(url, params=params)
            if response.status_code == 200:
                return response.json()
            else:
                print(f"âŒ Etherscan APIé”™è¯¯: {response.status_code}")
                return None
        except Exception as e:
            print(f"âŒ Etherscan APIè¯·æ±‚å¤±è´¥: {e}")
            return None
    
    def fetch_dune_data(self):
        """è·å–Duneå¤§é¢ç¨³å®šå¸è½¬è´¦æ•°æ®"""
        print("ğŸ”„ è·å–Duneå¤§é¢ç¨³å®šå¸æ•°æ®...")
        
        # ä½¿ç”¨æ­£ç¡®çš„Dune APIè°ƒç”¨æ–¹å¼
        whale_addresses = [
            "0x3f5ce5fbfe3e9af3971dd833d26ba9b5c936f0be",  # Binance Hot Wallet
            "0x28c6c06298d514db089934071355e5743bf21d60",  # Binance Cold Wallet
        ]
        
        all_activities = []
        
        for address in whale_addresses:
            print(f"  ğŸ” è·å–åœ°å€æ´»åŠ¨: {address[:10]}...")
            
            # è·å–Activityæ•°æ®
            activity_data = self.fetch_dune_activity(address, limit=100)
            if activity_data and 'activity' in activity_data:
                for activity in activity_data['activity']:
                    # è¿‡æ»¤å¤§é¢è½¬è´¦ï¼ˆ>1M USDï¼‰
                    if activity.get('value_usd', 0) > 1000000:
                        all_activities.append({
                            'timestamp': activity.get('block_time'),
                            'token': activity.get('token_metadata', {}).get('symbol', 'Unknown'),
                            'amount_usd': activity.get('value_usd', 0),
                            'from_address': activity.get('from'),
                            'to_address': address,
                            'tx_hash': activity.get('tx_hash'),
                            'type': activity.get('type'),
                            'asset_type': activity.get('asset_type'),
                            'chain_id': activity.get('chain_id')
                        })
            
            # APIé™é€Ÿ
            time.sleep(1)
        
        if all_activities:
            df_dune = pd.DataFrame(all_activities)
            df_dune['timestamp'] = pd.to_datetime(df_dune['timestamp'], utc=True)
            df_dune['date'] = df_dune['timestamp'].dt.date
            df_dune['hour'] = df_dune['timestamp'].dt.hour
            df_dune['weekday'] = df_dune['timestamp'].dt.weekday
            df_dune['month'] = df_dune['timestamp'].dt.month
            df_dune['year'] = df_dune['timestamp'].dt.year
            
            print(f"âœ… Duneæ•°æ®è·å–å®Œæˆ: {len(df_dune)} æ¡è®°å½•")
        else:
            df_dune = pd.DataFrame()
            print("âš ï¸  Duneæ•°æ®ä¸ºç©º")
            
        return df_dune
    
    def fetch_etherscan_data(self):
        """è·å–Etherscanå¤§é¢ETHè½¬è´¦æ•°æ®"""
        print("ğŸ”„ è·å–Etherscanå¤§é¢ETHæ•°æ®...")
        
        # å®šä¹‰è¦ç›‘æ§çš„é“¾
        chains = [1]  # åªæŸ¥ä¸»ç½‘ï¼ŒL2è·³è¿‡
        
        # å®šä¹‰è¦ç›‘æ§çš„å¤§é¢åœ°å€
        whale_addresses = [
            "0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045",
            "0x28C6c06298d514Db089934071355E5743bf21d60",
            "0x21a31Ee1afC51d94C2eFcCAa2092aD1028285549"
        ]
        
        all_balances = []
        
        for address in whale_addresses:
            print(f"  ğŸ” è·å–åœ°å€ä½™é¢: {address[:10]}...")
            
            for chain_id in chains:
                balance_data = self.fetch_etherscan_balance(address, chain_id)
                if balance_data and 'result' in balance_data:
                    try:
                        balance_eth = float(balance_data['result']) / 1e18
                        if balance_eth > 500:  # è¿‡æ»¤å¤§é¢ä½™é¢
                            all_balances.append({
                                'timestamp': datetime.now().isoformat(),
                                'amount_eth': balance_eth,
                                'address': address,
                                'chain_id': chain_id,
                                'amount_usd': balance_eth * 2000  # å‡è®¾ETHä»·æ ¼
                            })
                    except Exception as e:
                        print(f"    âš ï¸  è·³è¿‡æ— æ•ˆä½™é¢: {balance_data['result']}")
                        continue
                else:
                    print(f"    âš ï¸  APIè¿”å›å¼‚å¸¸: {balance_data}")
                # APIé™é€Ÿ
                time.sleep(0.2)
        
        if all_balances:
            df_etherscan = pd.DataFrame(all_balances)
            df_etherscan['timestamp'] = pd.to_datetime(df_etherscan['timestamp'], utc=True)
            df_etherscan['date'] = df_etherscan['timestamp'].dt.date
            df_etherscan['hour'] = df_etherscan['timestamp'].dt.hour
            df_etherscan['weekday'] = df_etherscan['timestamp'].dt.weekday
            df_etherscan['month'] = df_etherscan['timestamp'].dt.month
            df_etherscan['year'] = df_etherscan['timestamp'].dt.year
            
            print(f"âœ… Etherscanæ•°æ®è·å–å®Œæˆ: {len(df_etherscan)} æ¡è®°å½•")
        else:
            df_etherscan = pd.DataFrame()
            print("âš ï¸  Etherscanæ•°æ®ä¸ºç©º")
            
        return df_etherscan
    
    def integrate_whale_signals(self, df_dune, df_etherscan):
        """æ•´åˆå¤§é¢è½¬è´¦ä¿¡å·"""
        print("ğŸ”„ æ•´åˆå¤§é¢è½¬è´¦ä¿¡å·...")
        
        whale_signals = []
        
        # å¤„ç†Duneæ•°æ®
        if len(df_dune) > 0:
            for _, row in df_dune.iterrows():
                whale_signals.append({
                    'timestamp': row['timestamp'],
                    'signal_type': 'stablecoin_whale' if row['asset_type'] == 'erc20' else 'eth_whale',
                    'token': row['token'],
                    'amount': row['amount_usd'],
                    'amount_usd': row['amount_usd'],
                    'source': 'dune',
                    'chain_id': row.get('chain_id', 1),
                    'tx_hash': row.get('tx_hash'),
                    'date': row['date'],
                    'hour': row['hour'],
                    'weekday': row['weekday'],
                    'month': row['month'],
                    'year': row['year']
                })
        
        # å¤„ç†Etherscanæ•°æ®
        if len(df_etherscan) > 0:
            for _, row in df_etherscan.iterrows():
                whale_signals.append({
                    'timestamp': row['timestamp'],
                    'signal_type': 'eth_whale',
                    'token': 'ETH',
                    'amount': row['amount_eth'],
                    'amount_usd': row['amount_usd'],
                    'source': 'etherscan',
                    'chain_id': row['chain_id'],
                    'tx_hash': None,
                    'date': row['date'],
                    'hour': row['hour'],
                    'weekday': row['weekday'],
                    'month': row['month'],
                    'year': row['year']
                })
        
        if whale_signals:
            df_whale = pd.DataFrame(whale_signals)
            df_whale = df_whale.sort_values('timestamp').reset_index(drop=True)
            print(f"âœ… å¤§é¢ä¿¡å·æ•´åˆå®Œæˆ: {len(df_whale)} æ¡è®°å½•")
        else:
            df_whale = pd.DataFrame()
            print("âš ï¸  æ²¡æœ‰å¤§é¢ä¿¡å·æ•°æ®")
            
        return df_whale
    
    def save_processed_data(self, df_5m, df_15m, df_1h, df_4h, df_whale):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        print("ğŸ”„ ä¿å­˜å¤„ç†åçš„æ•°æ®...")
        
        # ä¿å­˜Kçº¿æ•°æ®
        if df_5m is not None:
            df_5m.to_parquet(f"{self.processed_path}/merged_5m_2023_2025.parquet", index=False)
            print(f"âœ… 5mæ•°æ®å·²ä¿å­˜: {len(df_5m)} è¡Œ")
            
        if df_15m is not None:
            df_15m.to_parquet(f"{self.processed_path}/merged_15m_2023_2025.parquet", index=False)
            print(f"âœ… 15mæ•°æ®å·²ä¿å­˜: {len(df_15m)} è¡Œ")
            
        if df_1h is not None:
            df_1h.to_parquet(f"{self.processed_path}/merged_1h_2023_2025.parquet", index=False)
            print(f"âœ… 1hæ•°æ®å·²ä¿å­˜: {len(df_1h)} è¡Œ")
            
        if df_4h is not None:
            df_4h.to_parquet(f"{self.processed_path}/merged_4h_2023_2025.parquet", index=False)
            print(f"âœ… 4hæ•°æ®å·²ä¿å­˜: {len(df_4h)} è¡Œ")
        
        # ä¿å­˜å¤§é¢ä¿¡å·æ•°æ®
        if len(df_whale) > 0:
            df_whale.to_parquet(f"{self.w1_path}/w1_2023_2025.parquet", index=False)
            print(f"âœ… å¤§é¢ä¿¡å·æ•°æ®å·²ä¿å­˜: {len(df_whale)} è¡Œ")
        else:
            print("âš ï¸  æ²¡æœ‰å¤§é¢ä¿¡å·æ•°æ®å¯ä¿å­˜")
    
    def run_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®ç®¡é“"""
        print("ğŸš€ å¼€å§‹æ•°æ®ç®¡é“å¤„ç†...")
        print("=" * 60)
        
        # 1. åˆå¹¶Kçº¿æ•°æ®
        df_5m = self.merge_kline_data("5m")
        df_15m = self.merge_kline_data("15m")
        df_1h = self.merge_kline_data("1h")
        df_4h = self.merge_kline_data("4h")
        
        # 2. å¤„ç†Kçº¿æ•°æ®
        df_5m = self.process_kline_data(df_5m, "5m")
        df_15m = self.process_kline_data(df_15m, "15m")
        df_1h = self.process_kline_data(df_1h, "1h")
        df_4h = self.process_kline_data(df_4h, "4h")
        
        # 3. è·å–å¤§é¢è½¬è´¦æ•°æ®
        df_dune = self.fetch_dune_data()
        df_etherscan = self.fetch_etherscan_data()
        
        # 4. æ•´åˆå¤§é¢ä¿¡å·
        df_whale = self.integrate_whale_signals(df_dune, df_etherscan)
        
        # 5. ä¿å­˜å¤„ç†åçš„æ•°æ®
        self.save_processed_data(df_5m, df_15m, df_1h, df_4h, df_whale)
        
        print("=" * 60)
        print("âœ… æ•°æ®ç®¡é“å¤„ç†å®Œæˆï¼")
        
        # è¿”å›æ•°æ®ç»Ÿè®¡
        stats = {
            '5m_rows': len(df_5m) if df_5m is not None else 0,
            '15m_rows': len(df_15m) if df_15m is not None else 0,
            '1h_rows': len(df_1h) if df_1h is not None else 0,
            '4h_rows': len(df_4h) if df_4h is not None else 0,
            'whale_signals': len(df_whale)
        }
        
        print("\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        for key, value in stats.items():
            print(f"  {key}: {value:,}")
            
        return stats

def main():
    """ä¸»å‡½æ•°"""
    pipeline = DataPipeline()
    stats = pipeline.run_pipeline()
    
    print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°:")
    print(f"  Kçº¿æ•°æ®: {pipeline.processed_path}")
    print(f"  å¤§é¢ä¿¡å·: {pipeline.w1_path}")

if __name__ == "__main__":
    main() 