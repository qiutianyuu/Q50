#!/usr/bin/env python3
"""
XGBoostäºŒåˆ†ç±»è®­ç»ƒ - ä½¿ç”¨å¹³è¡¡æ ‡ç­¾
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.metrics import roc_auc_score, classification_report
from sklearn.calibration import IsotonicRegression
import warnings
from pathlib import Path
import argparse
warnings.filterwarnings('ignore')

def load_data(features_path, labels_path):
    """åŠ è½½ç‰¹å¾å’Œæ ‡ç­¾æ•°æ®"""
    print(f"ğŸ“ åŠ è½½ç‰¹å¾æ•°æ®: {features_path}")
    features_df = pd.read_parquet(features_path)
    
    print(f"ğŸ“ åŠ è½½æ ‡ç­¾æ•°æ®: {labels_path}")
    labels_df = pd.read_csv(labels_path)
    
    # ç¡®ä¿æ—¶é—´æˆ³æ ¼å¼ä¸€è‡´
    features_df['timestamp'] = pd.to_datetime(features_df['timestamp'])
    labels_df['timestamp'] = pd.to_datetime(labels_df['timestamp'])
    
    # å°†æ ‡ç­¾æ—¶é—´æˆ³è½¬æ¢ä¸ºUTCæ—¶åŒº
    if labels_df['timestamp'].dt.tz is None:
        labels_df['timestamp'] = labels_df['timestamp'].dt.tz_localize('UTC')
    
    print(f"ç‰¹å¾æ—¶é—´æˆ³æ ¼å¼: {features_df['timestamp'].dtype}")
    print(f"æ ‡ç­¾æ—¶é—´æˆ³æ ¼å¼: {labels_df['timestamp'].dtype}")
    
    # åˆå¹¶ç‰¹å¾å’Œæ ‡ç­¾
    merged_df = features_df.merge(labels_df[['timestamp', 'label']], on='timestamp', how='inner')
    
    print(f"ğŸ“Š åˆå¹¶åæ•°æ®å½¢çŠ¶: {merged_df.shape}")
    print(f"ğŸ“… æ—¶é—´èŒƒå›´: {merged_df['timestamp'].min()} åˆ° {merged_df['timestamp'].max()}")
    
    # æ˜¾ç¤ºæ ‡ç­¾åˆ†å¸ƒ
    print(f"ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ:")
    print(merged_df['label'].value_counts())
    print(merged_df['label'].value_counts(normalize=True) * 100)
    
    return merged_df

def prepare_features(df, exclude_cols=None):
    """å‡†å¤‡ç‰¹å¾çŸ©é˜µ"""
    if exclude_cols is None:
        exclude_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'date', 'label']
    
    # æ’é™¤ä¸éœ€è¦çš„åˆ—
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    
    # åªä¿ç•™æ•°å€¼å‹ç‰¹å¾
    numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns
    print(f"ğŸ“ˆ ä½¿ç”¨ç‰¹å¾æ•°é‡: {len(numeric_cols)}")
    
    X = df[numeric_cols].fillna(0)
    y = df['label']
    
    return X, y, numeric_cols

def train_model(X_train, y_train, X_test, y_test, feature_cols):
    """è®­ç»ƒæ¨¡å‹"""
    print(f"ğŸ¯ è®­ç»ƒXGBoostæ¨¡å‹")
    print(f"è®­ç»ƒæ ·æœ¬: {len(X_train)}")
    print(f"æµ‹è¯•æ ·æœ¬: {len(X_test)}")
    
    # åªä¿ç•™äº¤æ˜“ä¿¡å·ï¼ˆå¤šå¤´å’Œç©ºå¤´ï¼‰
    train_trade_mask = y_train != 0
    test_trade_mask = y_test != 0
    
    X_train_trade = X_train[train_trade_mask]
    y_train_trade = y_train[train_trade_mask]
    X_test_trade = X_test[test_trade_mask]
    y_test_trade = y_test[test_trade_mask]
    
    # å°†æ ‡ç­¾è½¬æ¢ä¸ºäºŒåˆ†ç±»ï¼ˆ1=å¤šå¤´ï¼Œ0=ç©ºå¤´ï¼‰
    y_train_binary = (y_train_trade == 1).astype(int)
    y_test_binary = (y_test_trade == 1).astype(int)
    
    print(f"äº¤æ˜“ä¿¡å·è®­ç»ƒæ ·æœ¬: {len(X_train_trade)}")
    print(f"äº¤æ˜“ä¿¡å·æµ‹è¯•æ ·æœ¬: {len(X_test_trade)}")
    print(f"å¤šå¤´/ç©ºå¤´æ¯”ä¾‹: {y_train_binary.mean():.2%}")
    
    # è®­ç»ƒæ¨¡å‹
    model = xgb.XGBClassifier(
        max_depth=4,
        n_estimators=200,
        learning_rate=0.1,
        reg_alpha=0.1,
        reg_lambda=1.0,
        colsample_bytree=0.8,
        subsample=0.8,
        random_state=42,
        eval_metric='auc',
        early_stopping_rounds=50
    )
    
    # è®­ç»ƒ
    model.fit(
        X_train_trade, y_train_binary,
        eval_set=[(X_test_trade, y_test_binary)],
        verbose=0
    )
    
    # é¢„æµ‹
    train_proba = model.predict_proba(X_train_trade)[:, 1]
    test_proba = model.predict_proba(X_test_trade)[:, 1]
    
    # è¯„ä¼°
    train_auc = roc_auc_score(y_train_binary, train_proba)
    test_auc = roc_auc_score(y_test_binary, test_proba)
    
    # æ¦‚ç‡æ ¡å‡†
    calibrator = IsotonicRegression(out_of_bounds='clip')
    calibrator.fit(train_proba, y_train_binary)
    test_proba_calibrated = calibrator.predict(test_proba)
    test_auc_calibrated = roc_auc_score(y_test_binary, test_proba_calibrated)
    
    # ç‰¹å¾é‡è¦æ€§
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"è®­ç»ƒAUC: {train_auc:.4f}")
    print(f"æµ‹è¯•AUC: {test_auc:.4f}")
    print(f"æ ¡å‡†åAUC: {test_auc_calibrated:.4f}")
    print(f"è¿‡æ‹Ÿåˆç¨‹åº¦: {train_auc - test_auc:.4f}")
    print(f"Topç‰¹å¾: {', '.join(feature_importance.head(3)['feature'].tolist())}")
    
    return model, calibrator, feature_importance, {
        'train_auc': train_auc,
        'test_auc': test_auc,
        'test_auc_calibrated': test_auc_calibrated,
        'overfitting': train_auc - test_auc
    }

def save_model(model, calibrator, feature_importance, output_path):
    """ä¿å­˜æ¨¡å‹"""
    import joblib
    
    # ä¿å­˜æ¨¡å‹å’Œæ ¡å‡†å™¨
    model_data = {
        'model': model,
        'calibrator': calibrator,
        'feature_importance': feature_importance
    }
    
    joblib.dump(model_data, output_path)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {output_path}")

def main():
    parser = argparse.ArgumentParser(description='XGBoostäºŒåˆ†ç±»è®­ç»ƒ')
    parser.add_argument('--features', type=str, required=True, help='ç‰¹å¾æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--labels', type=str, required=True, help='æ ‡ç­¾æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, required=True, help='æ¨¡å‹è¾“å‡ºè·¯å¾„')
    parser.add_argument('--test_size', type=float, default=0.2, help='æµ‹è¯•é›†æ¯”ä¾‹')
    
    args = parser.parse_args()
    
    print("ğŸš€ XGBoostäºŒåˆ†ç±»è®­ç»ƒ")
    print(f"ğŸ“ ç‰¹å¾æ–‡ä»¶: {args.features}")
    print(f"ğŸ“ æ ‡ç­¾æ–‡ä»¶: {args.labels}")
    print(f"ğŸ“ æ¨¡å‹è¾“å‡º: {args.output}")
    print(f"ğŸ“Š æµ‹è¯•é›†æ¯”ä¾‹: {args.test_size}")
    
    # åŠ è½½æ•°æ®
    df = load_data(args.features, args.labels)
    
    # å‡†å¤‡ç‰¹å¾
    X, y, feature_cols = prepare_features(df)
    
    # æ—¶é—´åºåˆ—åˆ†å‰²
    split_idx = int(len(X) * (1 - args.test_size))
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]
    
    print(f"æ—¶é—´åˆ†å‰²ç‚¹: {df['timestamp'].iloc[split_idx]}")
    
    # è®­ç»ƒæ¨¡å‹
    model, calibrator, feature_importance, metrics = train_model(
        X_train, y_train, X_test, y_test, feature_cols
    )
    
    # ä¿å­˜æ¨¡å‹
    save_model(model, calibrator, feature_importance, args.output)
    
    print("âœ… è®­ç»ƒå®Œæˆï¼")

if __name__ == "__main__":
    main() 