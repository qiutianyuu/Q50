#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆXGBoost Walk-Forwardè®­ç»ƒ - ä½¿ç”¨æˆæœ¬æ„ŸçŸ¥æ ‡ç­¾å’Œå¢å¼ºç‰¹å¾
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.metrics import roc_auc_score, classification_report
from sklearn.calibration import IsotonicRegression
import warnings
from pathlib import Path
import argparse
warnings.filterwarnings('ignore')

def load_data(features_path, labels_path):
    """åŠ è½½ç‰¹å¾å’Œæ ‡ç­¾æ•°æ®"""
    print(f"ğŸ“ åŠ è½½ç‰¹å¾æ•°æ®: {features_path}")
    features_df = pd.read_parquet(features_path)
    
    print(f"ğŸ“ åŠ è½½æ ‡ç­¾æ•°æ®: {labels_path}")
    labels_df = pd.read_csv(labels_path)
    
    # ç¡®ä¿æ—¶é—´æˆ³æ ¼å¼ä¸€è‡´
    features_df['timestamp'] = pd.to_datetime(features_df['timestamp'])
    labels_df['timestamp'] = pd.to_datetime(labels_df['timestamp'])
    
    # åˆå¹¶ç‰¹å¾å’Œæ ‡ç­¾
    merged_df = features_df.merge(labels_df[['timestamp', 'label']], on='timestamp', how='inner')
    
    print(f"ğŸ“Š åˆå¹¶åæ•°æ®å½¢çŠ¶: {merged_df.shape}")
    print(f"ğŸ“… æ—¶é—´èŒƒå›´: {merged_df['timestamp'].min()} åˆ° {merged_df['timestamp'].max()}")
    
    return merged_df

def prepare_features(df, exclude_cols=None):
    """å‡†å¤‡ç‰¹å¾çŸ©é˜µ"""
    if exclude_cols is None:
        exclude_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'date', 'label']
    
    # æ’é™¤ä¸éœ€è¦çš„åˆ—
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    
    # åªä¿ç•™æ•°å€¼å‹ç‰¹å¾
    numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns
    print(f"ğŸ“ˆ ä½¿ç”¨ç‰¹å¾æ•°é‡: {len(numeric_cols)}")
    
    X = df[numeric_cols].fillna(0)
    y = df['label']
    
    return X, y, numeric_cols

def walk_forward_validation(df, train_days=180, test_days=30, step_days=30):
    """Walk-ForwardéªŒè¯"""
    print(f"ğŸ”„ Walk-ForwardéªŒè¯: {train_days}å¤©è®­ç»ƒ, {test_days}å¤©æµ‹è¯•, {step_days}å¤©æ­¥é•¿")
    
    # æŒ‰æ—¶é—´æ’åº
    df = df.sort_values('timestamp').reset_index(drop=True)
    
    # è®¡ç®—æ—¶é—´çª—å£
    start_date = df['timestamp'].min()
    end_date = df['timestamp'].max()
    
    # ç”Ÿæˆæ—¶é—´çª—å£
    windows = []
    current_start = start_date
    while current_start + pd.Timedelta(days=train_days + test_days) <= end_date:
        train_start = current_start
        train_end = train_start + pd.Timedelta(days=train_days)
        test_start = train_end
        test_end = test_start + pd.Timedelta(days=test_days)
        
        windows.append({
            'train_start': train_start,
            'train_end': train_end,
            'test_start': test_start,
            'test_end': test_end
        })
        
        current_start += pd.Timedelta(days=step_days)
    
    print(f"ğŸ“Š ç”Ÿæˆ {len(windows)} ä¸ªæ—¶é—´çª—å£")
    
    results = []
    
    for i, window in enumerate(windows):
        print(f"\nğŸ”„ çª—å£ {i+1}/{len(windows)}")
        print(f"è®­ç»ƒ: {window['train_start'].date()} - {window['train_end'].date()}")
        print(f"æµ‹è¯•: {window['test_start'].date()} - {window['test_end'].date()}")
        
        # åˆ†å‰²æ•°æ®
        train_mask = (df['timestamp'] >= window['train_start']) & (df['timestamp'] < window['train_end'])
        test_mask = (df['timestamp'] >= window['test_start']) & (df['timestamp'] < window['test_end'])
        
        train_df = df[train_mask]
        test_df = df[test_mask]
        
        if len(train_df) < 1000 or len(test_df) < 100:
            print("âš ï¸ æ ·æœ¬ä¸è¶³ï¼Œè·³è¿‡")
            continue
        
        # å‡†å¤‡ç‰¹å¾
        X_train, y_train, feature_cols = prepare_features(train_df)
        X_test, y_test, _ = prepare_features(test_df)
        
        # åªä¿ç•™äº¤æ˜“ä¿¡å·
        train_trade_mask = y_train != -1
        test_trade_mask = y_test != -1
        
        if train_trade_mask.sum() < 500 or test_trade_mask.sum() < 50:
            print("âš ï¸ äº¤æ˜“ä¿¡å·ä¸è¶³ï¼Œè·³è¿‡")
            continue
        
        X_train_trade = X_train[train_trade_mask]
        y_train_trade = y_train[train_trade_mask]
        X_test_trade = X_test[test_trade_mask]
        y_test_trade = y_test[test_trade_mask]
        
        print(f"è®­ç»ƒæ ·æœ¬: {len(X_train_trade)} (äº¤æ˜“ä¿¡å·: {train_trade_mask.sum()})")
        print(f"æµ‹è¯•æ ·æœ¬: {len(X_test_trade)} (äº¤æ˜“ä¿¡å·: {test_trade_mask.sum()})")
        
        # è®­ç»ƒæ¨¡å‹
        model = xgb.XGBClassifier(
            max_depth=4,
            n_estimators=200,
            learning_rate=0.1,
            reg_alpha=0.1,
            reg_lambda=1.0,
            colsample_bytree=0.8,
            subsample=0.8,
            random_state=42,
            eval_metric='auc',
            early_stopping_rounds=50
        )
        
        # è®­ç»ƒ
        model.fit(
            X_train_trade, y_train_trade,
            eval_set=[(X_test_trade, y_test_trade)],
            verbose=0
        )
        
        # é¢„æµ‹
        train_proba = model.predict_proba(X_train_trade)[:, 1]
        test_proba = model.predict_proba(X_test_trade)[:, 1]
        
        # è¯„ä¼°
        train_auc = roc_auc_score(y_train_trade, train_proba)
        test_auc = roc_auc_score(y_test_trade, test_proba)
        
        # æ¦‚ç‡æ ¡å‡†
        calibrator = IsotonicRegression(out_of_bounds='clip')
        calibrator.fit(train_proba, y_train_trade)
        test_proba_calibrated = calibrator.predict(test_proba)
        test_auc_calibrated = roc_auc_score(y_test_trade, test_proba_calibrated)
        
        # ç‰¹å¾é‡è¦æ€§
        feature_importance = pd.DataFrame({
            'feature': feature_cols,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        result = {
            'window': i + 1,
            'train_start': window['train_start'],
            'train_end': window['train_end'],
            'test_start': window['test_start'],
            'test_end': window['test_end'],
            'train_samples': len(X_train_trade),
            'test_samples': len(X_test_trade),
            'train_auc': train_auc,
            'test_auc': test_auc,
            'test_auc_calibrated': test_auc_calibrated,
            'overfitting': train_auc - test_auc,
            'top_features': feature_importance.head(10)['feature'].tolist(),
            'model': model,
            'calibrator': calibrator
        }
        
        results.append(result)
        
        print(f"è®­ç»ƒAUC: {train_auc:.4f}")
        print(f"æµ‹è¯•AUC: {test_auc:.4f}")
        print(f"æ ¡å‡†åAUC: {test_auc_calibrated:.4f}")
        print(f"è¿‡æ‹Ÿåˆç¨‹åº¦: {train_auc - test_auc:.4f}")
        print(f"Topç‰¹å¾: {', '.join(feature_importance.head(3)['feature'].tolist())}")
    
    return results

def analyze_results(results):
    """åˆ†æWalk-Forwardç»“æœ"""
    if not results:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆç»“æœ")
        return
    
    print(f"\nğŸ“Š Walk-Forwardç»“æœåˆ†æ")
    print(f"æœ‰æ•ˆçª—å£æ•°: {len(results)}")
    
    # ç»Ÿè®¡æŒ‡æ ‡
    test_aucs = [r['test_auc'] for r in results]
    test_aucs_calibrated = [r['test_auc_calibrated'] for r in results]
    overfitting = [r['overfitting'] for r in results]
    
    print(f"\nğŸ“ˆ AUCç»Ÿè®¡:")
    print(f"æµ‹è¯•AUCå‡å€¼: {np.mean(test_aucs):.4f} Â± {np.std(test_aucs):.4f}")
    print(f"æ ¡å‡†åAUCå‡å€¼: {np.mean(test_aucs_calibrated):.4f} Â± {np.std(test_aucs_calibrated):.4f}")
    print(f"è¿‡æ‹Ÿåˆå‡å€¼: {np.mean(overfitting):.4f} Â± {np.std(overfitting):.4f}")
    
    print(f"\nğŸ“Š ç¨³å®šæ€§åˆ†æ:")
    print(f"AUC > 0.55çš„çª—å£: {sum(1 for auc in test_aucs if auc > 0.55)}/{len(test_aucs)}")
    print(f"AUC > 0.57çš„çª—å£: {sum(1 for auc in test_aucs if auc > 0.57)}/{len(test_aucs)}")
    print(f"è¿‡æ‹Ÿåˆ < 0.05çš„çª—å£: {sum(1 for of in overfitting if of < 0.05)}/{len(overfitting)}")
    
    # ç‰¹å¾é‡è¦æ€§ç»Ÿè®¡
    all_features = []
    for result in results:
        all_features.extend(result['top_features'][:5])
    
    feature_counts = pd.Series(all_features).value_counts()
    print(f"\nğŸ† æœ€å¸¸å‡ºç°çš„é‡è¦ç‰¹å¾:")
    for feature, count in feature_counts.head(10).items():
        print(f"  {feature}: {count}æ¬¡")
    
    return results

def save_results(results, output_path):
    """ä¿å­˜ç»“æœ"""
    if not results:
        return
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_df = pd.DataFrame([
        {
            'window': r['window'],
            'train_start': r['train_start'],
            'train_end': r['train_end'],
            'test_start': r['test_start'],
            'test_end': r['test_end'],
            'train_samples': r['train_samples'],
            'test_samples': r['test_samples'],
            'train_auc': r['train_auc'],
            'test_auc': r['test_auc'],
            'test_auc_calibrated': r['test_auc_calibrated'],
            'overfitting': r['overfitting']
        }
        for r in results
    ])
    
    results_df.to_csv(output_path, index=False)
    print(f"âœ… ç»“æœå·²ä¿å­˜: {output_path}")

def main():
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆXGBoost Walk-Forwardè®­ç»ƒ')
    parser.add_argument('--features', type=str, required=True, help='ç‰¹å¾æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--labels', type=str, required=True, help='æ ‡ç­¾æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default='walk_forward_results_enhanced.csv', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--train_days', type=int, default=180, help='è®­ç»ƒå¤©æ•°')
    parser.add_argument('--test_days', type=int, default=30, help='æµ‹è¯•å¤©æ•°')
    parser.add_argument('--step_days', type=int, default=30, help='æ­¥é•¿å¤©æ•°')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¢å¼ºç‰ˆXGBoost Walk-Forwardè®­ç»ƒ")
    print(f"ğŸ“ ç‰¹å¾æ–‡ä»¶: {args.features}")
    print(f"ğŸ“ æ ‡ç­¾æ–‡ä»¶: {args.labels}")
    print(f"â±ï¸ è®­ç»ƒå¤©æ•°: {args.train_days}")
    print(f"â±ï¸ æµ‹è¯•å¤©æ•°: {args.test_days}")
    print(f"â±ï¸ æ­¥é•¿å¤©æ•°: {args.step_days}")
    
    # åŠ è½½æ•°æ®
    df = load_data(args.features, args.labels)
    
    # Walk-ForwardéªŒè¯
    results = walk_forward_validation(df, args.train_days, args.test_days, args.step_days)
    
    # åˆ†æç»“æœ
    analyze_results(results)
    
    # ä¿å­˜ç»“æœ
    save_results(results, args.output)

if __name__ == "__main__":
    main() 