#!/usr/bin/env python3
"""
RexKing â€“ Enhanced Model Training with Order Flow Features

æ•´åˆè®¢å•æµç‰¹å¾é‡æ–°è®­ç»ƒæ¨¡å‹ï¼Œæå‡é¢„æµ‹èƒ½åŠ›
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, classification_report
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# ---------- é…ç½® ----------
DATA_DIR = Path("/Users/qiutianyu/data/processed")
FEATURES_FILE = DATA_DIR / "features_15m_enhanced.parquet"
ORDERFLOW_FILE = Path("data/mid_features_15m_orderflow.parquet")
MODEL_FILE = "xgb_15m_enhanced.bin"

def load_and_merge_data():
    """åŠ è½½å¹¶åˆå¹¶ç‰¹å¾æ•°æ®"""
    print("ğŸ“¥ åŠ è½½åŸºç¡€ç‰¹å¾æ•°æ®...")
    df = pd.read_parquet(FEATURES_FILE)
    print(f"åŸºç¡€ç‰¹å¾æ•°æ®: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
    
    if ORDERFLOW_FILE.exists():
        print("ğŸ“Š åŠ è½½è®¢å•æµç‰¹å¾...")
        orderflow = pd.read_parquet(ORDERFLOW_FILE)
        orderflow['timestamp'] = pd.to_datetime(orderflow['timestamp'], utc=True)
        
        # ç¡®ä¿ä¸»æ•°æ®ä¹Ÿæœ‰UTCæ—¶åŒº
        df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True)
        
        # é€‰æ‹©é«˜ä¿¡æ¯å¯†åº¦çš„è®¢å•æµç‰¹å¾
        orderflow_cols = [
            'liquidity_pressure', 'liquidity_pressure_ma',
            'taker_imbalance', 'taker_imbalance_ma',
            'order_flow_intensity', 'order_flow_intensity_ma',
            'liquidity_impact', 'liquidity_impact_ma',
            'buy_pressure_ratio', 'sell_pressure_ratio',
            'order_flow_strength', 'order_flow_strength_ma',
            'liquidity_stress', 'liquidity_stress_ma',
            'spread_compression', 'volume_imbalance',
            'price_pressure', 'vwap_deviation'
        ]
        
        # åªä¿ç•™å­˜åœ¨çš„åˆ—
        available_cols = [col for col in orderflow_cols if col in orderflow.columns]
        print(f"å¯ç”¨è®¢å•æµç‰¹å¾: {len(available_cols)}")
        
        # åˆå¹¶è®¢å•æµç‰¹å¾
        df = pd.merge_asof(
            df.sort_values('timestamp'),
            orderflow[['timestamp'] + available_cols].sort_values('timestamp'),
            on='timestamp', direction='backward'
        )
        
        # å¡«å……NaN
        df[available_cols] = df[available_cols].fillna(0)
        
        print(f"âœ… è®¢å•æµç‰¹å¾æ•´åˆå®Œæˆï¼Œæ–°å¢ {len(available_cols)} ä¸ªç‰¹å¾")
        print(f"åˆå¹¶åæ•°æ®: {len(df)} è¡Œ, {len(df.columns)} åˆ—")
    else:
        print("âš ï¸ è®¢å•æµæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡è®¢å•æµç‰¹å¾")
    
    return df

def prepare_features(df):
    """å‡†å¤‡è®­ç»ƒç‰¹å¾"""
    # æ’é™¤éç‰¹å¾åˆ—
    exclude_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'label', 'trend_1h', 'trend_4h']
    feature_cols = [col for col in df.columns if col not in exclude_cols and pd.api.types.is_numeric_dtype(df[col])]
    
    X = df[feature_cols].fillna(0)
    y = df['label']
    
    print(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"æ ·æœ¬æ•°é‡: {len(X)}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {y.value_counts().to_dict()}")
    
    return X, y, feature_cols

def train_enhanced_model(X, y, feature_cols):
    """è®­ç»ƒå¢å¼ºç‰ˆæ¨¡å‹"""
    print("ğŸš€ è®­ç»ƒå¢å¼ºç‰ˆæ¨¡å‹...")
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    # ä¼˜åŒ–å‚æ•°
    params = {
        'max_depth': 6,
        'n_estimators': 300,
        'learning_rate': 0.1,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'min_child_weight': 3,
        'reg_alpha': 0.1,
        'reg_lambda': 1.0,
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'use_label_encoder': False,
        'random_state': 42
    }
    
    # è®­ç»ƒæ¨¡å‹
    model = xgb.XGBClassifier(**params)
    model.fit(X_train, y_train, verbose=0)
    
    # è¯„ä¼°
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    y_pred = model.predict(X_test)
    
    auc = roc_auc_score(y_test, y_pred_proba)
    print(f"æµ‹è¯•é›†AUC: {auc:.4f}")
    
    # é«˜ç½®ä¿¡åº¦é¢„æµ‹
    high_conf_mask = (y_pred_proba > 0.8) | (y_pred_proba < 0.2)
    if high_conf_mask.sum() > 0:
        high_conf_auc = roc_auc_score(y_test[high_conf_mask], y_pred_proba[high_conf_mask])
        high_conf_acc = (y_pred[high_conf_mask] == y_test[high_conf_mask]).mean()
        print(f"é«˜ç½®ä¿¡åº¦æ ·æœ¬AUC: {high_conf_auc:.4f}")
        print(f"é«˜ç½®ä¿¡åº¦æ ·æœ¬å‡†ç¡®ç‡: {high_conf_acc:.4f}")
        print(f"é«˜ç½®ä¿¡åº¦æ ·æœ¬æ•°é‡: {high_conf_mask.sum()}")
    
    # ç‰¹å¾é‡è¦æ€§
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print(f"\nğŸ“Š Top 15 ç‰¹å¾é‡è¦æ€§:")
    for i, row in feature_importance.head(15).iterrows():
        print(f"  {row['feature']}: {row['importance']:.4f}")
    
    # ä¿å­˜æ¨¡å‹
    model.save_model(MODEL_FILE)
    print(f"âœ… å¢å¼ºç‰ˆæ¨¡å‹å·²ä¿å­˜: {MODEL_FILE}")
    
    # ä¿å­˜ç‰¹å¾åˆ—è¡¨
    feature_info = {
        'feature_cols': feature_cols,
        'auc': auc,
        'high_conf_auc': high_conf_auc if high_conf_mask.sum() > 0 else 0,
        'high_conf_acc': high_conf_acc if high_conf_mask.sum() > 0 else 0
    }
    
    import json
    with open('enhanced_model_info.json', 'w') as f:
        json.dump(feature_info, f, indent=2, default=str)
    
    return model, auc, feature_importance

def main():
    print("=== RexKing Enhanced Model Training ===")
    
    # åŠ è½½å¹¶åˆå¹¶æ•°æ®
    df = load_and_merge_data()
    
    # å‡†å¤‡ç‰¹å¾
    X, y, feature_cols = prepare_features(df)
    
    # è®­ç»ƒæ¨¡å‹
    model, auc, feature_importance = train_enhanced_model(X, y, feature_cols)
    
    print(f"\nğŸ‰ å¢å¼ºç‰ˆæ¨¡å‹è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ç»ˆAUC: {auc:.4f}")
    print(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    
    return model, auc

if __name__ == "__main__":
    main() 