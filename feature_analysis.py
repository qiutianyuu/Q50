#!/usr/bin/env python3
"""
ç‰¹å¾åˆ†æå’Œç­›é€‰ - SHAPé‡è¦æ€§åˆ†æ
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
import shap
import warnings
warnings.filterwarnings('ignore')

def analyze_features_15m():
    """åˆ†æ15mç‰¹å¾"""
    print("ğŸ” åˆ†æ15mç‰¹å¾...")
    
    # è¯»å–æ•°æ®
    df = pd.read_parquet('/Users/qiutianyu/data/processed/features_15m_2023_2025.parquet')
    
    # å‡†å¤‡ç‰¹å¾
    exclude_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'label', 'trend_1h', 'trend_4h']
    feature_cols = [col for col in df.columns if col not in exclude_cols and pd.api.types.is_numeric_dtype(df[col])]
    
    X = df[feature_cols].fillna(0)
    y = df['label']
    
    print(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"æ ·æœ¬æ•°é‡: {len(X)}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {y.value_counts().to_dict()}")
    
    # è®­ç»ƒç®€å•æ¨¡å‹
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    model = xgb.XGBClassifier(
        max_depth=4, n_estimators=100, learning_rate=0.1,
        random_state=42, eval_metric='auc'
    )
    model.fit(X_train, y_train, verbose=0)
    
    # é¢„æµ‹å’Œè¯„ä¼°
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_pred_proba)
    print(f"æµ‹è¯•é›†AUC: {auc:.4f}")
    
    # SHAPåˆ†æ
    print("ğŸ“Š è®¡ç®—SHAPé‡è¦æ€§...")
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_test.sample(min(1000, len(X_test)), random_state=42))
    
    # è®¡ç®—ç‰¹å¾é‡è¦æ€§
    feature_importance = np.abs(shap_values).mean(axis=0)
    importance_df = pd.DataFrame({
        'feature': feature_cols,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    print("\nğŸ“ˆ Top-20 ç‰¹å¾é‡è¦æ€§:")
    print(importance_df.head(20).to_string(index=False))
    
    # ä¿å­˜ç»“æœ
    importance_df.to_csv('/Users/qiutianyu/data/processed/feature_importance_15m.csv', index=False)
    print(f"\nç‰¹å¾é‡è¦æ€§å·²ä¿å­˜: /Users/qiutianyu/data/processed/feature_importance_15m.csv")
    
    # ç­›é€‰é‡è¦ç‰¹å¾ï¼ˆé‡è¦æ€§ > 1%ï¼‰
    threshold = importance_df['importance'].max() * 0.01
    important_features = importance_df[importance_df['importance'] > threshold]['feature'].tolist()
    
    print(f"\nğŸ¯ é‡è¦ç‰¹å¾ç­›é€‰ (é˜ˆå€¼: {threshold:.6f}):")
    print(f"ä¿ç•™ç‰¹å¾æ•°: {len(important_features)} / {len(feature_cols)}")
    print(f"ä¿ç•™ç‰¹å¾: {important_features}")
    
    # ä¿å­˜é‡è¦ç‰¹å¾åˆ—è¡¨
    with open('/Users/qiutianyu/data/processed/important_features_15m.txt', 'w') as f:
        f.write('\n'.join(important_features))
    
    return important_features, importance_df

def analyze_features_5m():
    """åˆ†æ5mç‰¹å¾"""
    print("\nğŸ” åˆ†æ5mç‰¹å¾...")
    
    # è¯»å–æ•°æ®
    df = pd.read_parquet('/Users/qiutianyu/data/processed/features_5m_2023_2025.parquet')
    
    # å‡†å¤‡ç‰¹å¾
    exclude_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'label', 'trend_1h', 'trend_4h']
    feature_cols = [col for col in df.columns if col not in exclude_cols and pd.api.types.is_numeric_dtype(df[col])]
    
    X = df[feature_cols].fillna(0)
    y = df['label']
    
    print(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"æ ·æœ¬æ•°é‡: {len(X)}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {y.value_counts().to_dict()}")
    
    # è®­ç»ƒç®€å•æ¨¡å‹
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    model = xgb.XGBClassifier(
        max_depth=4, n_estimators=100, learning_rate=0.1,
        random_state=42, eval_metric='auc'
    )
    model.fit(X_train, y_train, verbose=0)
    
    # é¢„æµ‹å’Œè¯„ä¼°
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    auc = roc_auc_score(y_test, y_pred_proba)
    print(f"æµ‹è¯•é›†AUC: {auc:.4f}")
    
    # SHAPåˆ†æ
    print("ğŸ“Š è®¡ç®—SHAPé‡è¦æ€§...")
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_test.sample(min(1000, len(X_test)), random_state=42))
    
    # è®¡ç®—ç‰¹å¾é‡è¦æ€§
    feature_importance = np.abs(shap_values).mean(axis=0)
    importance_df = pd.DataFrame({
        'feature': feature_cols,
        'importance': feature_importance
    }).sort_values('importance', ascending=False)
    
    print("\nğŸ“ˆ Top-20 ç‰¹å¾é‡è¦æ€§:")
    print(importance_df.head(20).to_string(index=False))
    
    # ä¿å­˜ç»“æœ
    importance_df.to_csv('/Users/qiutianyu/data/processed/feature_importance_5m.csv', index=False)
    print(f"\nç‰¹å¾é‡è¦æ€§å·²ä¿å­˜: /Users/qiutianyu/data/processed/feature_importance_5m.csv")
    
    # ç­›é€‰é‡è¦ç‰¹å¾ï¼ˆé‡è¦æ€§ > 1%ï¼‰
    threshold = importance_df['importance'].max() * 0.01
    important_features = importance_df[importance_df['importance'] > threshold]['feature'].tolist()
    
    print(f"\nğŸ¯ é‡è¦ç‰¹å¾ç­›é€‰ (é˜ˆå€¼: {threshold:.6f}):")
    print(f"ä¿ç•™ç‰¹å¾æ•°: {len(important_features)} / {len(feature_cols)}")
    print(f"ä¿ç•™ç‰¹å¾: {important_features}")
    
    # ä¿å­˜é‡è¦ç‰¹å¾åˆ—è¡¨
    with open('/Users/qiutianyu/data/processed/important_features_5m.txt', 'w') as f:
        f.write('\n'.join(important_features))
    
    return important_features, importance_df

def main():
    print("ğŸš€ å¼€å§‹ç‰¹å¾åˆ†æ...")
    
    # åˆ†æ15mç‰¹å¾
    important_15m, importance_15m = analyze_features_15m()
    
    # åˆ†æ5mç‰¹å¾
    important_5m, importance_5m = analyze_features_5m()
    
    # å¯¹æ¯”åˆ†æ
    print("\nğŸ“Š ç‰¹å¾é‡è¦æ€§å¯¹æ¯”:")
    print("15m Top-10ç‰¹å¾:")
    print(importance_15m.head(10)[['feature', 'importance']].to_string(index=False))
    
    print("\n5m Top-10ç‰¹å¾:")
    print(importance_5m.head(10)[['feature', 'importance']].to_string(index=False))
    
    print("\nğŸ‰ ç‰¹å¾åˆ†æå®Œæˆ!")

if __name__ == "__main__":
    main() 