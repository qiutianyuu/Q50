#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆXGBoost Walk-Forwardè®­ç»ƒ - æ­£ç¡®å¤„ç†ä¸‰åˆ†ç±»æ ‡ç­¾
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.metrics import roc_auc_score, classification_report
from sklearn.calibration import IsotonicRegression
import warnings
from pathlib import Path
import argparse
warnings.filterwarnings('ignore')

def load_data(features_path, labels_path):
    """åŠ è½½ç‰¹å¾å’Œæ ‡ç­¾æ•°æ®"""
    print(f"ğŸ“ åŠ è½½ç‰¹å¾æ•°æ®: {features_path}")
    features_df = pd.read_parquet(features_path)
    
    print(f"ğŸ“ åŠ è½½æ ‡ç­¾æ•°æ®: {labels_path}")
    labels_df = pd.read_csv(labels_path)
    
    # ç¡®ä¿æ—¶é—´æˆ³æ ¼å¼ä¸€è‡´
    features_df['timestamp'] = pd.to_datetime(features_df['timestamp'])
    labels_df['timestamp'] = pd.to_datetime(labels_df['timestamp'], format='mixed')
    # å¼ºåˆ¶æ ‡ç­¾æ—¶é—´æˆ³ä¸ºUTC
    if labels_df['timestamp'].dt.tz is None:
        labels_df['timestamp'] = labels_df['timestamp'].dt.tz_localize('UTC')
    print(f"ç‰¹å¾æ—¶é—´æˆ³æ ¼å¼: {features_df['timestamp'].dtype}")
    print(f"æ ‡ç­¾æ—¶é—´æˆ³æ ¼å¼: {labels_df['timestamp'].dtype}")
    
    # åˆå¹¶ç‰¹å¾å’Œæ ‡ç­¾
    merged_df = features_df.merge(labels_df[['timestamp', 'label']], on='timestamp', how='inner')
    
    print(f"ğŸ“Š åˆå¹¶åæ•°æ®å½¢çŠ¶: {merged_df.shape}")
    print(f"ğŸ“… æ—¶é—´èŒƒå›´: {merged_df['timestamp'].min()} åˆ° {merged_df['timestamp'].max()}")
    
    # æ˜¾ç¤ºæ ‡ç­¾åˆ†å¸ƒ
    print(f"ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ:")
    print(merged_df['label'].value_counts())
    print(merged_df['label'].value_counts(normalize=True) * 100)
    
    return merged_df

def prepare_features(df, exclude_cols=None):
    """å‡†å¤‡ç‰¹å¾çŸ©é˜µ"""
    if exclude_cols is None:
        exclude_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'date', 'label']
    
    # æ’é™¤ä¸éœ€è¦çš„åˆ—
    feature_cols = [col for col in df.columns if col not in exclude_cols]
    
    # åªä¿ç•™æ•°å€¼å‹ç‰¹å¾
    numeric_cols = df[feature_cols].select_dtypes(include=[np.number]).columns
    print(f"ğŸ“ˆ ä½¿ç”¨ç‰¹å¾æ•°é‡: {len(numeric_cols)}")
    
    X = df[numeric_cols].fillna(0)
    y = df['label']
    
    # é‡æ–°æ˜ å°„æ ‡ç­¾ï¼š-1->0, 0->1, 1->2
    y = y.map({-1: 0, 0: 1, 1: 2})
    
    return X, y, numeric_cols

def walk_forward_validation(df, train_days=180, test_days=30, step_days=30):
    """Walk-ForwardéªŒè¯"""
    print(f"ğŸ”„ Walk-ForwardéªŒè¯: {train_days}å¤©è®­ç»ƒ, {test_days}å¤©æµ‹è¯•, {step_days}å¤©æ­¥é•¿")
    
    # æŒ‰æ—¶é—´æ’åº
    df = df.sort_values('timestamp').reset_index(drop=True)
    
    # è®¡ç®—æ—¶é—´çª—å£
    start_date = df['timestamp'].min()
    end_date = df['timestamp'].max()
    
    # ç”Ÿæˆæ—¶é—´çª—å£
    windows = []
    current_start = start_date
    while current_start + pd.Timedelta(days=train_days + test_days) <= end_date:
        train_start = current_start
        train_end = train_start + pd.Timedelta(days=train_days)
        test_start = train_end
        test_end = test_start + pd.Timedelta(days=test_days)
        
        windows.append({
            'train_start': train_start,
            'train_end': train_end,
            'test_start': test_start,
            'test_end': test_end
        })
        
        current_start += pd.Timedelta(days=step_days)
    
    print(f"ğŸ“Š ç”Ÿæˆ {len(windows)} ä¸ªæ—¶é—´çª—å£")
    
    results = []
    
    for i, window in enumerate(windows):
        print(f"\nğŸ”„ çª—å£ {i+1}/{len(windows)}")
        print(f"è®­ç»ƒ: {window['train_start'].date()} - {window['train_end'].date()}")
        print(f"æµ‹è¯•: {window['test_start'].date()} - {window['test_end'].date()}")
        
        # åˆ†å‰²æ•°æ®
        train_mask = (df['timestamp'] >= window['train_start']) & (df['timestamp'] < window['train_end'])
        test_mask = (df['timestamp'] >= window['test_start']) & (df['timestamp'] < window['test_end'])
        
        train_df = df[train_mask]
        test_df = df[test_mask]
        
        if len(train_df) < 1000 or len(test_df) < 100:
            print("âš ï¸ æ ·æœ¬ä¸è¶³ï¼Œè·³è¿‡")
            continue
        
        # å‡†å¤‡ç‰¹å¾
        X_train, y_train, feature_cols = prepare_features(train_df)
        X_test, y_test, _ = prepare_features(test_df)
        
        # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
        print(f"è®­ç»ƒæ ‡ç­¾åˆ†å¸ƒ: {y_train.value_counts().to_dict()}")
        print(f"æµ‹è¯•æ ‡ç­¾åˆ†å¸ƒ: {y_test.value_counts().to_dict()}")
        
        # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ ·æœ¬
        if len(y_train.unique()) < 2 or len(y_test.unique()) < 2:
            print("âš ï¸ æ ‡ç­¾ç±»åˆ«ä¸è¶³ï¼Œè·³è¿‡")
            continue
        
        # è®­ç»ƒæ¨¡å‹ - ä½¿ç”¨ä¸‰åˆ†ç±»
        model = xgb.XGBClassifier(
            max_depth=4,
            n_estimators=200,
            learning_rate=0.1,
            reg_alpha=0.1,
            reg_lambda=1.0,
            colsample_bytree=0.8,
            subsample=0.8,
            random_state=42,
            eval_metric='mlogloss',  # å¤šåˆ†ç±»ä½¿ç”¨mlogloss
            early_stopping_rounds=50
        )
        
        # è®­ç»ƒ
        model.fit(
            X_train, y_train,
            eval_set=[(X_test, y_test)],
            verbose=0
        )
        
        # é¢„æµ‹
        train_proba = model.predict_proba(X_train)
        test_proba = model.predict_proba(X_test)
        
        # å¯¹äºä¸‰åˆ†ç±»ï¼Œæˆ‘ä»¬éœ€è¦è®¡ç®—æ¯ä¸ªç±»åˆ«çš„AUC
        # è¿™é‡Œæˆ‘ä»¬ä¸»è¦å…³æ³¨å¤šå¤´(1)å’Œç©ºå¤´(-1)çš„é¢„æµ‹
        if 1 in y_train.unique() and -1 in y_train.unique():
            # å¤šå¤´vså…¶ä»–
            y_train_long = (y_train == 1).astype(int)
            y_test_long = (y_test == 1).astype(int)
            long_proba = test_proba[:, 1] if test_proba.shape[1] > 1 else test_proba[:, 0]
            long_auc = roc_auc_score(y_test_long, long_proba)
            
            # ç©ºå¤´vså…¶ä»–
            y_train_short = (y_test == -1).astype(int)
            y_test_short = (y_test == -1).astype(int)
            short_proba = test_proba[:, 2] if test_proba.shape[1] > 2 else test_proba[:, 0]
            short_auc = roc_auc_score(y_test_short, short_proba)
            
            test_auc = (long_auc + short_auc) / 2
        else:
            test_auc = 0.5
        
        # ç‰¹å¾é‡è¦æ€§
        feature_importance = pd.DataFrame({
            'feature': feature_cols,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        result = {
            'window': i + 1,
            'train_start': window['train_start'],
            'train_end': window['train_end'],
            'test_start': window['test_start'],
            'test_end': window['test_end'],
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'test_auc': test_auc,
            'top_features': feature_importance.head(10)['feature'].tolist(),
            'model': model
        }
        
        results.append(result)
        
        print(f"æµ‹è¯•AUC: {test_auc:.4f}")
        print(f"Topç‰¹å¾: {', '.join(feature_importance.head(3)['feature'].tolist())}")
    
    return results

def analyze_results(results):
    """åˆ†æWalk-Forwardç»“æœ"""
    if not results:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆç»“æœ")
        return
    
    print(f"\nğŸ“Š Walk-Forwardç»“æœåˆ†æ")
    print(f"æœ‰æ•ˆçª—å£æ•°: {len(results)}")
    
    # ç»Ÿè®¡æŒ‡æ ‡
    test_aucs = [r['test_auc'] for r in results]
    
    print(f"\nğŸ“ˆ AUCç»Ÿè®¡:")
    print(f"æµ‹è¯•AUCå‡å€¼: {np.mean(test_aucs):.4f} Â± {np.std(test_aucs):.4f}")
    
    print(f"\nğŸ“Š ç¨³å®šæ€§åˆ†æ:")
    print(f"AUC > 0.55çš„çª—å£: {sum(1 for auc in test_aucs if auc > 0.55)}/{len(test_aucs)}")
    print(f"AUC > 0.57çš„çª—å£: {sum(1 for auc in test_aucs if auc > 0.57)}/{len(test_aucs)}")
    
    # ç‰¹å¾é‡è¦æ€§ç»Ÿè®¡
    all_features = []
    for result in results:
        all_features.extend(result['top_features'][:5])
    
    feature_counts = pd.Series(all_features).value_counts()
    print(f"\nğŸ† æœ€å¸¸å‡ºç°çš„é‡è¦ç‰¹å¾:")
    for feature, count in feature_counts.head(10).items():
        print(f"  {feature}: {count}æ¬¡")
    
    return results

def save_results(results, output_path):
    """ä¿å­˜ç»“æœ"""
    if not results:
        return
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_df = pd.DataFrame([
        {
            'window': r['window'],
            'train_start': r['train_start'],
            'train_end': r['train_end'],
            'test_start': r['test_start'],
            'test_end': r['test_end'],
            'train_samples': r['train_samples'],
            'test_samples': r['test_samples'],
            'test_auc': r['test_auc']
        }
        for r in results
    ])
    
    results_df.to_csv(output_path, index=False)
    print(f"âœ… ç»“æœå·²ä¿å­˜: {output_path}")

def main():
    parser = argparse.ArgumentParser(description='ä¿®å¤ç‰ˆXGBoost Walk-Forwardè®­ç»ƒ')
    parser.add_argument('--features', type=str, required=True, help='ç‰¹å¾æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--labels', type=str, required=True, help='æ ‡ç­¾æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default='walk_forward_results_fixed.csv', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--train_days', type=int, default=180, help='è®­ç»ƒå¤©æ•°')
    parser.add_argument('--test_days', type=int, default=30, help='æµ‹è¯•å¤©æ•°')
    parser.add_argument('--step_days', type=int, default=30, help='æ­¥é•¿å¤©æ•°')
    
    args = parser.parse_args()
    
    print("ğŸš€ ä¿®å¤ç‰ˆXGBoost Walk-Forwardè®­ç»ƒ")
    print(f"ğŸ“ ç‰¹å¾æ–‡ä»¶: {args.features}")
    print(f"ğŸ“ æ ‡ç­¾æ–‡ä»¶: {args.labels}")
    print(f"â±ï¸ è®­ç»ƒå¤©æ•°: {args.train_days}")
    print(f"â±ï¸ æµ‹è¯•å¤©æ•°: {args.test_days}")
    print(f"â±ï¸ æ­¥é•¿å¤©æ•°: {args.step_days}")
    
    # åŠ è½½æ•°æ®
    df = load_data(args.features, args.labels)
    
    # Walk-ForwardéªŒè¯
    results = walk_forward_validation(df, args.train_days, args.test_days, args.step_days)
    
    # åˆ†æç»“æœ
    analyze_results(results)
    
    # ä¿å­˜ç»“æœ
    save_results(results, args.output)

if __name__ == "__main__":
    main() 