#!/usr/bin/env python3
"""
å¿«é€Ÿè®­ç»ƒä¼˜åŒ–åçš„æ¨¡å‹ - ä½¿ç”¨ç»éªŒæ€§å‚æ•°
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, classification_report
import warnings
warnings.filterwarnings('ignore')

def train_optimized_15m():
    """è®­ç»ƒä¼˜åŒ–åçš„15mæ¨¡å‹"""
    print("ğŸš€ è®­ç»ƒä¼˜åŒ–åçš„15mæ¨¡å‹...")
    
    # è¯»å–æ•°æ®
    df = pd.read_parquet('/Users/qiutianyu/data/processed/features_15m_2023_2025.parquet')
    
    # å‡†å¤‡ç‰¹å¾
    exclude_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'label', 'trend_1h', 'trend_4h']
    feature_cols = [col for col in df.columns if col not in exclude_cols and pd.api.types.is_numeric_dtype(df[col])]
    
    X = df[feature_cols].fillna(0)
    y = df['label']
    
    print(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"æ ·æœ¬æ•°é‡: {len(X)}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {y.value_counts().to_dict()}")
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    # ä¼˜åŒ–å‚æ•°ï¼ˆç»éªŒæ€§ï¼‰
    params = {
        'max_depth': 6,
        'n_estimators': 300,
        'learning_rate': 0.1,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'min_child_weight': 3,
        'reg_alpha': 0.1,
        'reg_lambda': 1.0,
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'use_label_encoder': False,
        'random_state': 42
    }
    
    # è®­ç»ƒæ¨¡å‹
    model = xgb.XGBClassifier(**params)
    model.fit(X_train, y_train, verbose=0)
    
    # è¯„ä¼°
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    y_pred = model.predict(X_test)
    
    auc = roc_auc_score(y_test, y_pred_proba)
    print(f"æµ‹è¯•é›†AUC: {auc:.4f}")
    
    # é«˜ç½®ä¿¡åº¦é¢„æµ‹
    high_conf_mask = (y_pred_proba > 0.8) | (y_pred_proba < 0.2)
    if high_conf_mask.sum() > 0:
        high_conf_auc = roc_auc_score(y_test[high_conf_mask], y_pred_proba[high_conf_mask])
        high_conf_acc = (y_pred[high_conf_mask] == y_test[high_conf_mask]).mean()
        print(f"é«˜ç½®ä¿¡åº¦æ ·æœ¬AUC: {high_conf_auc:.4f}")
        print(f"é«˜ç½®ä¿¡åº¦æ ·æœ¬å‡†ç¡®ç‡: {high_conf_acc:.4f}")
        print(f"é«˜ç½®ä¿¡åº¦æ ·æœ¬æ•°é‡: {high_conf_mask.sum()}")
    
    # ä¿å­˜æ¨¡å‹
    model.save_model('xgb_15m_optimized.bin')
    print("âœ… ä¼˜åŒ–åçš„15mæ¨¡å‹å·²ä¿å­˜: xgb_15m_optimized.bin")
    
    return model, auc

def train_optimized_5m():
    """è®­ç»ƒä¼˜åŒ–åçš„5mæ¨¡å‹"""
    print("\nğŸš€ è®­ç»ƒä¼˜åŒ–åçš„5mæ¨¡å‹...")
    
    # è¯»å–æ•°æ®
    df = pd.read_parquet('/Users/qiutianyu/data/processed/features_5m_2023_2025.parquet')
    
    # å‡†å¤‡ç‰¹å¾
    exclude_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'label', 'trend_1h', 'trend_4h']
    feature_cols = [col for col in df.columns if col not in exclude_cols and pd.api.types.is_numeric_dtype(df[col])]
    
    X = df[feature_cols].fillna(0)
    y = df['label']
    
    print(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"æ ·æœ¬æ•°é‡: {len(X)}")
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {y.value_counts().to_dict()}")
    
    # åˆ†å‰²æ•°æ®
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    # ä¼˜åŒ–å‚æ•°ï¼ˆç»éªŒæ€§ï¼‰
    params = {
        'max_depth': 5,
        'n_estimators': 250,
        'learning_rate': 0.12,
        'subsample': 0.85,
        'colsample_bytree': 0.85,
        'min_child_weight': 2,
        'reg_alpha': 0.05,
        'reg_lambda': 0.8,
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'use_label_encoder': False,
        'random_state': 42
    }
    
    # è®­ç»ƒæ¨¡å‹
    model = xgb.XGBClassifier(**params)
    model.fit(X_train, y_train, verbose=0)
    
    # è¯„ä¼°
    y_pred_proba = model.predict_proba(X_test)[:, 1]
    y_pred = model.predict(X_test)
    
    auc = roc_auc_score(y_test, y_pred_proba)
    print(f"æµ‹è¯•é›†AUC: {auc:.4f}")
    
    # é«˜ç½®ä¿¡åº¦é¢„æµ‹
    high_conf_mask = (y_pred_proba > 0.8) | (y_pred_proba < 0.2)
    if high_conf_mask.sum() > 0:
        high_conf_auc = roc_auc_score(y_test[high_conf_mask], y_pred_proba[high_conf_mask])
        high_conf_acc = (y_pred[high_conf_mask] == y_test[high_conf_mask]).mean()
        print(f"é«˜ç½®ä¿¡åº¦æ ·æœ¬AUC: {high_conf_auc:.4f}")
        print(f"é«˜ç½®ä¿¡åº¦æ ·æœ¬å‡†ç¡®ç‡: {high_conf_acc:.4f}")
        print(f"é«˜ç½®ä¿¡åº¦æ ·æœ¬æ•°é‡: {high_conf_mask.sum()}")
    
    # ä¿å­˜æ¨¡å‹
    model.save_model('xgb_5m_optimized.bin')
    print("âœ… ä¼˜åŒ–åçš„5mæ¨¡å‹å·²ä¿å­˜: xgb_5m_optimized.bin")
    
    return model, auc

def main():
    print("ğŸ¯ å¿«é€Ÿè®­ç»ƒä¼˜åŒ–åçš„æ¨¡å‹...")
    
    # è®­ç»ƒ15mæ¨¡å‹
    model_15m, auc_15m = train_optimized_15m()
    
    # è®­ç»ƒ5mæ¨¡å‹
    model_5m, auc_5m = train_optimized_5m()
    
    # ç»“æœæ€»ç»“
    print("\nğŸ“Š è®­ç»ƒç»“æœæ€»ç»“:")
    print(f"15mæ¨¡å‹AUC: {auc_15m:.4f}")
    print(f"5mæ¨¡å‹AUC: {auc_5m:.4f}")
    
    print("\nğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆ!")

if __name__ == "__main__":
    main() 