#!/usr/bin/env python3
"""
ç¦»çº¿å¾®è§‚ç‰¹å¾åˆ†æè„šæœ¬
æ‰¹é‡å¤„ç†WebSocketæ•°æ®ï¼Œè®¡ç®—ä¿¡æ¯ç³»æ•°ï¼Œè®­ç»ƒç®€å•æ¨¡å‹éªŒè¯alpha
"""

import os
import glob
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import logging
from typing import Dict, List, Tuple, Optional
import argparse
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class OfflineMicroFeatureAnalysis:
    def __init__(self, symbol: str = "ETH-USDT", window_size: int = 50):
        self.symbol = symbol
        self.window_size = window_size
        self.websocket_dir = "data/websocket"
        self.output_dir = "data/analysis"
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
        
    def load_all_websocket_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """åŠ è½½æ‰€æœ‰WebSocketæ•°æ®"""
        logger.info("ğŸ“Š åŠ è½½æ‰€æœ‰WebSocketæ•°æ®...")
        
        # åŠ è½½æ‰€æœ‰OrderBookæ•°æ®
        orderbook_files = glob.glob(f"{self.websocket_dir}/orderbook_{self.symbol}_*.parquet")
        orderbook_files.sort()
        
        orderbook_dfs = []
        for file in orderbook_files:
            df = pd.read_parquet(file)
            if 'ts' in df.columns:
                df['timestamp'] = pd.to_datetime(df['ts'], unit='ms')
            orderbook_dfs.append(df)
        
        orderbook_df = pd.concat(orderbook_dfs, ignore_index=True)
        orderbook_df = orderbook_df.sort_values('timestamp').reset_index(drop=True)
        
        # åŠ è½½æ‰€æœ‰Tradesæ•°æ®
        trades_files = glob.glob(f"{self.websocket_dir}/trades_{self.symbol}_*.parquet")
        trades_files.sort()
        
        trades_dfs = []
        for file in trades_files:
            df = pd.read_parquet(file)
            if 'ts' in df.columns:
                df['timestamp'] = pd.to_datetime(df['ts'], unit='ms')
            trades_dfs.append(df)
        
        trades_df = pd.concat(trades_dfs, ignore_index=True)
        trades_df = trades_df.sort_values('timestamp').reset_index(drop=True)
        
        logger.info(f"ğŸ“ˆ OrderBookæ•°æ®: {len(orderbook_df)} æ¡")
        logger.info(f"ğŸ“Š Tradesæ•°æ®: {len(trades_df)} æ¡")
        
        return orderbook_df, trades_df
    
    def calculate_micro_price_features_batch(self, orderbook_df: pd.DataFrame) -> pd.DataFrame:
        """æ‰¹é‡è®¡ç®—å¾®ä»·æ ¼ç‰¹å¾"""
        logger.info("ğŸ“ˆ è®¡ç®—å¾®ä»·æ ¼ç‰¹å¾...")
        
        features = []
        
        for _, row in orderbook_df.iterrows():
            # ç›´æ¥ä½¿ç”¨è§£æå¥½çš„OrderBookæ•°æ®
            bid_prices = []
            bid_sizes = []
            ask_prices = []
            ask_sizes = []
            
            # æå–5æ¡£ä¹°å–ç›˜æ•°æ®
            for i in range(1, 6):
                bid_price_col = f'bid{i}_price'
                bid_size_col = f'bid{i}_size'
                ask_price_col = f'ask{i}_price'
                ask_size_col = f'ask{i}_size'
                
                if bid_price_col in row and bid_size_col in row:
                    bid_prices.append(row[bid_price_col])
                    bid_sizes.append(row[bid_size_col])
                
                if ask_price_col in row and ask_size_col in row:
                    ask_prices.append(row[ask_price_col])
                    ask_sizes.append(row[ask_size_col])
            
            if not bid_prices or not ask_prices:
                continue
            
            # åŠ æƒå¹³å‡ä»·æ ¼ (VWAP)
            bid_vwap = np.average(bid_prices, weights=bid_sizes) if bid_sizes else 0
            ask_vwap = np.average(ask_prices, weights=ask_sizes) if ask_sizes else 0
            
            # ä¸­é—´ä»·
            mid_price = (bid_prices[0] + ask_prices[0]) / 2 if bid_prices and ask_prices else 0
            
            # ä¹°å–ä»·å·®
            spread = ask_prices[0] - bid_prices[0] if bid_prices and ask_prices else 0
            spread_bps = (spread / mid_price * 10000) if mid_price > 0 else 0
            
            # è®¢å•ç°¿ä¸å¹³è¡¡
            bid_volume = sum(bid_sizes)
            ask_volume = sum(ask_sizes)
            volume_imbalance = (bid_volume - ask_volume) / (bid_volume + ask_volume) if (bid_volume + ask_volume) > 0 else 0
            
            # ä»·æ ¼å‹åŠ›
            price_pressure = (ask_vwap - mid_price) / mid_price if mid_price > 0 else 0
            
            features.append({
                'timestamp': row.get('timestamp', pd.Timestamp.now()),
                'mid_price': mid_price,
                'bid_vwap': bid_vwap,
                'ask_vwap': ask_vwap,
                'spread': spread,
                'spread_bps': spread_bps,
                'volume_imbalance': volume_imbalance,
                'price_pressure': price_pressure,
                'bid_volume': bid_volume,
                'ask_volume': ask_volume,
                'total_volume': bid_volume + ask_volume
            })
        
        return pd.DataFrame(features)
    
    def calculate_order_flow_features_batch(self, trades_df: pd.DataFrame) -> pd.DataFrame:
        """æ‰¹é‡è®¡ç®—è®¢å•æµç‰¹å¾"""
        logger.info("ğŸ“Š è®¡ç®—è®¢å•æµç‰¹å¾...")
        
        features = []
        
        # æŒ‰æ—¶é—´çª—å£åˆ†ç»„è®¡ç®—ç‰¹å¾
        trades_df = trades_df.sort_values('timestamp')
        
        for i in range(0, len(trades_df), self.window_size):
            window_trades = trades_df.iloc[i:i+self.window_size]
            
            if len(window_trades) == 0:
                continue
            
            # è®¡ç®—åŸºç¡€ç»Ÿè®¡
            prices = window_trades['price'].astype(float)
            sizes = window_trades['size'].astype(float)
            sides = window_trades['side'].astype(str)
            
            # ä»·æ ¼ç‰¹å¾
            price_mean = prices.mean()
            price_std = prices.std()
            price_range = prices.max() - prices.min()
            
            # æˆäº¤é‡ç‰¹å¾
            total_volume = sizes.sum()
            avg_trade_size = sizes.mean()
            large_trades = (sizes > sizes.quantile(0.9)).sum()
            
            # ä¹°å–å‹åŠ›
            buy_volume = sizes[sides == 'buy'].sum()
            sell_volume = sizes[sides == 'sell'].sum()
            buy_ratio = buy_volume / (buy_volume + sell_volume) if (buy_volume + sell_volume) > 0 else 0.5
            
            # ä»·æ ¼åŠ¨é‡
            if len(prices) > 1:
                price_momentum = (prices.iloc[-1] - prices.iloc[0]) / prices.iloc[0]
            else:
                price_momentum = 0
            
            # æˆäº¤é‡åŠ æƒå¹³å‡ä»·æ ¼
            vwap = np.average(prices, weights=sizes)
            
            # æ—¶é—´ç‰¹å¾
            time_span = (window_trades['timestamp'].max() - window_trades['timestamp'].min()).total_seconds()
            trade_frequency = len(window_trades) / time_span if time_span > 0 else 0
            
            features.append({
                'timestamp': window_trades['timestamp'].iloc[-1],
                'price_mean': price_mean,
                'price_std': price_std,
                'price_range': price_range,
                'total_volume': total_volume,
                'avg_trade_size': avg_trade_size,
                'large_trades': large_trades,
                'buy_ratio': buy_ratio,
                'price_momentum': price_momentum,
                'vwap': vwap,
                'trade_frequency': trade_frequency,
                'trade_count': len(window_trades)
            })
        
        return pd.DataFrame(features)
    
    def generate_labels(self, features_df: pd.DataFrame, forward_period: int = 5) -> pd.DataFrame:
        """ç”Ÿæˆæ ‡ç­¾ï¼šæœªæ¥ä»·æ ¼å˜åŒ–"""
        logger.info(f"ğŸ·ï¸ ç”Ÿæˆæ ‡ç­¾ (å‰å‘{forward_period}æœŸ)...")
        
        # è®¡ç®—æœªæ¥ä»·æ ¼å˜åŒ–
        features_df = features_df.sort_values('timestamp').reset_index(drop=True)
        
        # æœªæ¥ä»·æ ¼å˜åŒ–
        features_df['future_price_change'] = features_df['mid_price'].shift(-forward_period) / features_df['mid_price'] - 1
        
        # äºŒåˆ†ç±»æ ‡ç­¾
        features_df['label_binary'] = (features_df['future_price_change'] > 0).astype(int)
        
        # ä¸‰åˆ†ç±»æ ‡ç­¾ (ä¸Šæ¶¨/ä¸‹è·Œ/éœ‡è¡) - å…ˆä¸astype(int)
        features_df['label_three'] = pd.cut(
            features_df['future_price_change'], 
            bins=[-np.inf, -0.001, 0.001, np.inf], 
            labels=[0, 1, 2]
        )
        
        # ç§»é™¤æœ€åå‡ è¡Œï¼ˆæ²¡æœ‰æœªæ¥æ•°æ®ï¼‰
        features_df = features_df.dropna(subset=['future_price_change', 'label_three']).reset_index(drop=True)
        features_df['label_three'] = features_df['label_three'].astype(int)
        
        logger.info(f"ğŸ“Š æ ‡ç­¾ç»Ÿè®¡:")
        logger.info(f"  äºŒåˆ†ç±»: {features_df['label_binary'].value_counts().to_dict()}")
        logger.info(f"  ä¸‰åˆ†ç±»: {features_df['label_three'].value_counts().to_dict()}")
        logger.info(f"  ä»·æ ¼å˜åŒ–å‡å€¼: {features_df['future_price_change'].mean():.6f}")
        logger.info(f"  ä»·æ ¼å˜åŒ–æ ‡å‡†å·®: {features_df['future_price_change'].std():.6f}")
        
        return features_df
    
    def calculate_information_coefficient(self, features_df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—ä¿¡æ¯ç³»æ•° (IC)"""
        logger.info("ğŸ“Š è®¡ç®—ä¿¡æ¯ç³»æ•°...")
        
        # é€‰æ‹©æ•°å€¼å‹ç‰¹å¾
        numeric_features = features_df.select_dtypes(include=[np.number]).columns.tolist()
        numeric_features = [f for f in numeric_features if f not in [
            'future_price_change', 'label_binary', 'label_three'
        ]]
        
        ic_results = []
        
        for feature in numeric_features:
            # ç§»é™¤ç¼ºå¤±å€¼
            valid_data = features_df[[feature, 'future_price_change']].dropna()
            
            if len(valid_data) < 10:  # è‡³å°‘éœ€è¦10ä¸ªæœ‰æ•ˆæ•°æ®ç‚¹
                continue
            
            # è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°
            pearson_corr, pearson_p = stats.pearsonr(valid_data[feature], valid_data['future_price_change'])
            
            # è®¡ç®—æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•°
            spearman_corr, spearman_p = stats.spearmanr(valid_data[feature], valid_data['future_price_change'])
            
            ic_results.append({
                'feature': feature,
                'pearson_ic': pearson_corr,
                'pearson_p': pearson_p,
                'spearman_ic': spearman_corr,
                'spearman_p': spearman_p,
                'abs_pearson_ic': abs(pearson_corr),
                'abs_spearman_ic': abs(spearman_corr),
                'sample_size': len(valid_data)
            })
        
        ic_df = pd.DataFrame(ic_results)
        ic_df = ic_df.sort_values('abs_pearson_ic', ascending=False)
        
        logger.info(f"ğŸ“ˆ ICåˆ†æå®Œæˆï¼Œå…±{len(ic_df)}ä¸ªç‰¹å¾")
        logger.info(f"ğŸ† å‰5ä¸ªæœ€å¼ºç‰¹å¾:")
        for i, row in ic_df.head().iterrows():
            logger.info(f"  {row['feature']}: Pearson={row['pearson_ic']:.4f}, Spearman={row['spearman_ic']:.4f}")
        
        return ic_df
    
    def train_simple_model(self, features_df: pd.DataFrame, top_features: int = 10) -> Dict:
        """è®­ç»ƒç®€å•æ¨¡å‹"""
        logger.info(f"ğŸ¤– è®­ç»ƒç®€å•æ¨¡å‹ (ä½¿ç”¨å‰{top_features}ä¸ªç‰¹å¾)...")
        
        try:
            from sklearn.model_selection import cross_val_score, StratifiedKFold
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.metrics import accuracy_score, classification_report
            from sklearn.preprocessing import StandardScaler
            
            # è·å–ICæœ€é«˜çš„ç‰¹å¾
            ic_df = self.calculate_information_coefficient(features_df)
            top_feature_names = ic_df.head(top_features)['feature'].tolist()
            
            # å‡†å¤‡æ•°æ®
            X = features_df[top_feature_names].fillna(0)
            y_binary = features_df['label_binary']
            y_three = features_df['label_three']
            
            # æ ‡å‡†åŒ–
            scaler = StandardScaler()
            X_scaled = scaler.fit_transform(X)
            
            # äºŒåˆ†ç±»æ¨¡å‹
            rf_binary = RandomForestClassifier(n_estimators=100, random_state=42)
            cv_scores_binary = cross_val_score(rf_binary, X_scaled, y_binary, cv=5, scoring='accuracy')
            
            # ä¸‰åˆ†ç±»æ¨¡å‹
            rf_three = RandomForestClassifier(n_estimators=100, random_state=42)
            cv_scores_three = cross_val_score(rf_three, X_scaled, y_three, cv=5, scoring='accuracy')
            
            # ç‰¹å¾é‡è¦æ€§
            rf_binary.fit(X_scaled, y_binary)
            feature_importance = pd.DataFrame({
                'feature': top_feature_names,
                'importance': rf_binary.feature_importances_
            }).sort_values('importance', ascending=False)
            
            results = {
                'binary_accuracy_mean': cv_scores_binary.mean(),
                'binary_accuracy_std': cv_scores_binary.std(),
                'three_accuracy_mean': cv_scores_three.mean(),
                'three_accuracy_std': cv_scores_three.std(),
                'feature_importance': feature_importance,
                'top_features': top_feature_names,
                'cv_scores_binary': cv_scores_binary,
                'cv_scores_three': cv_scores_three
            }
            
            logger.info(f"ğŸ“Š æ¨¡å‹ç»“æœ:")
            logger.info(f"  äºŒåˆ†ç±»å‡†ç¡®ç‡: {results['binary_accuracy_mean']:.4f} Â± {results['binary_accuracy_std']:.4f}")
            logger.info(f"  ä¸‰åˆ†ç±»å‡†ç¡®ç‡: {results['three_accuracy_mean']:.4f} Â± {results['three_accuracy_std']:.4f}")
            logger.info(f"ğŸ† å‰5ä¸ªé‡è¦ç‰¹å¾:")
            for i, row in feature_importance.head().iterrows():
                logger.info(f"  {row['feature']}: {row['importance']:.4f}")
            
            return results
            
        except ImportError:
            logger.warning("âŒ sklearnæœªå®‰è£…ï¼Œè·³è¿‡æ¨¡å‹è®­ç»ƒ")
            return {}
    
    def save_results(self, features_df: pd.DataFrame, ic_df: pd.DataFrame, model_results: Dict):
        """ä¿å­˜åˆ†æç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜ç‰¹å¾æ•°æ®
        features_file = f"{self.output_dir}/micro_features_{self.symbol}_{timestamp}.parquet"
        features_df.to_parquet(features_file, index=False)
        logger.info(f"ğŸ’¾ ä¿å­˜ç‰¹å¾æ•°æ®: {features_file}")
        
        # ä¿å­˜ICç»“æœ
        ic_file = f"{self.output_dir}/ic_analysis_{self.symbol}_{timestamp}.csv"
        ic_df.to_csv(ic_file, index=False)
        logger.info(f"ğŸ’¾ ä¿å­˜ICåˆ†æ: {ic_file}")
        
        # ä¿å­˜æ¨¡å‹ç»“æœ
        if model_results:
            model_file = f"{self.output_dir}/model_results_{self.symbol}_{timestamp}.json"
            import json
            # è½¬æ¢numpyæ•°ç»„ä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
            model_results_json = {
                'binary_accuracy_mean': float(model_results['binary_accuracy_mean']),
                'binary_accuracy_std': float(model_results['binary_accuracy_std']),
                'three_accuracy_mean': float(model_results['three_accuracy_mean']),
                'three_accuracy_std': float(model_results['three_accuracy_std']),
                'top_features': model_results['top_features'],
                'cv_scores_binary': model_results['cv_scores_binary'].tolist(),
                'cv_scores_three': model_results['cv_scores_three'].tolist(),
                'feature_importance': model_results['feature_importance'].to_dict('records')
            }
            with open(model_file, 'w') as f:
                json.dump(model_results_json, f, indent=2)
            logger.info(f"ğŸ’¾ ä¿å­˜æ¨¡å‹ç»“æœ: {model_file}")
    
    def run_analysis(self, forward_period: int = 5, top_features: int = 10) -> Dict:
        """è¿è¡Œå®Œæ•´åˆ†ææµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹ç¦»çº¿å¾®è§‚ç‰¹å¾åˆ†æ...")
        
        # 1. åŠ è½½æ•°æ®
        orderbook_df, trades_df = self.load_all_websocket_data()
        
        # 2. è®¡ç®—ç‰¹å¾
        micro_price_features = self.calculate_micro_price_features_batch(orderbook_df)
        order_flow_features = self.calculate_order_flow_features_batch(trades_df)
        
        # 3. åˆå¹¶ç‰¹å¾
        features_df = pd.merge_asof(
            micro_price_features.sort_values('timestamp'),
            order_flow_features.sort_values('timestamp'),
            on='timestamp',
            direction='nearest',
            suffixes=('', '_flow')
        )
        
        # 4. è®¡ç®—è¡ç”Ÿç‰¹å¾
        features_df['price_change'] = features_df['mid_price'].pct_change()
        features_df['price_change_abs'] = features_df['price_change'].abs()
        features_df['volume_change'] = features_df['total_volume'].pct_change()
        features_df['buy_pressure_change'] = features_df['buy_ratio'].diff()
        features_df['spread_change'] = features_df['spread'].pct_change()
        features_df['price_volatility'] = features_df['price_change'].rolling(5).std()
        features_df['price_trend'] = features_df['mid_price'].rolling(10).mean()
        features_df['trend_deviation'] = (features_df['mid_price'] - features_df['price_trend']) / features_df['price_trend']
        
        logger.info(f"ğŸ“Š åˆå¹¶åç‰¹å¾æ•°æ®: {len(features_df)} æ¡, {len(features_df.columns)} åˆ—")
        
        # 5. ç”Ÿæˆæ ‡ç­¾
        features_df = self.generate_labels(features_df, forward_period)
        
        # 6. è®¡ç®—IC
        ic_df = self.calculate_information_coefficient(features_df)
        
        # 7. è®­ç»ƒæ¨¡å‹
        model_results = self.train_simple_model(features_df, top_features)
        
        # 8. ä¿å­˜ç»“æœ
        self.save_results(features_df, ic_df, model_results)
        
        logger.info("âœ… ç¦»çº¿åˆ†æå®Œæˆï¼")
        
        return {
            'features_df': features_df,
            'ic_df': ic_df,
            'model_results': model_results
        }

def main():
    parser = argparse.ArgumentParser(description="ç¦»çº¿å¾®è§‚ç‰¹å¾åˆ†æ")
    parser.add_argument("--symbol", default="ETH-USDT", help="äº¤æ˜“å¯¹")
    parser.add_argument("--window-size", type=int, default=50, help="æ»‘åŠ¨çª—å£å¤§å°")
    parser.add_argument("--forward-period", type=int, default=5, help="å‰å‘é¢„æµ‹æœŸæ•°")
    parser.add_argument("--top-features", type=int, default=10, help="ä½¿ç”¨å‰Nä¸ªç‰¹å¾è®­ç»ƒæ¨¡å‹")
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆ†æå®ä¾‹
    analyzer = OfflineMicroFeatureAnalysis(
        symbol=args.symbol,
        window_size=args.window_size
    )
    
    # è¿è¡Œåˆ†æ
    results = analyzer.run_analysis(
        forward_period=args.forward_period,
        top_features=args.top_features
    )
    
    # æ‰“å°æ€»ç»“
    print("\n" + "="*50)
    print("ğŸ“Š åˆ†ææ€»ç»“")
    print("="*50)
    print(f"æ•°æ®é‡: {len(results['features_df'])} æ¡")
    print(f"ç‰¹å¾æ•°: {len(results['features_df'].columns)} ä¸ª")
    
    if results['model_results']:
        print(f"äºŒåˆ†ç±»å‡†ç¡®ç‡: {results['model_results']['binary_accuracy_mean']:.4f}")
        print(f"ä¸‰åˆ†ç±»å‡†ç¡®ç‡: {results['model_results']['three_accuracy_mean']:.4f}")
    
    print("="*50)

if __name__ == "__main__":
    main() 