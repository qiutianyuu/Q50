#!/usr/bin/env python3
"""
æ•°æ®æ³„æ¼è¯Šæ–­ - æ‰¾å‡ºè¿‡æ‹Ÿåˆçš„æ ¹æœ¬åŸå› 
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
import warnings
warnings.filterwarnings('ignore')

def diagnose_leakage():
    """è¯Šæ–­æ•°æ®æ³„æ¼"""
    print("ğŸ” æ•°æ®æ³„æ¼è¯Šæ–­...")
    
    # è¯»å–æ•°æ®
    df = pd.read_parquet('/Users/qiutianyu/data/processed/features_15m_fixed.parquet')
    
    # å‡†å¤‡ç‰¹å¾
    exclude_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'label', 'trend_1h', 'trend_4h']
    feature_cols = [col for col in df.columns if col not in exclude_cols and pd.api.types.is_numeric_dtype(df[col])]
    
    X = df[feature_cols].fillna(0)
    y = df['label']
    
    print(f"ç‰¹å¾æ•°é‡: {len(feature_cols)}")
    print(f"æ ·æœ¬æ•°é‡: {len(X)}")
    
    # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    print(f"\nğŸ“Š æ ‡ç­¾åˆ†å¸ƒ:")
    print(f"åšå¤šä¿¡å· (1): {y.sum()} ({y.mean():.2%})")
    print(f"åšç©ºä¿¡å· (0): {(y==0).sum()} ({(y==0).mean():.2%})")
    
    # æ£€æŸ¥æ—¶é—´åˆ†å¸ƒ
    df_sorted = df.sort_values('timestamp')
    print(f"\nğŸ“… æ—¶é—´åˆ†å¸ƒ:")
    print(f"å¼€å§‹æ—¶é—´: {df_sorted['timestamp'].min()}")
    print(f"ç»“æŸæ—¶é—´: {df_sorted['timestamp'].max()}")
    print(f"æ—¶é—´è·¨åº¦: {(df_sorted['timestamp'].max() - df_sorted['timestamp'].min()).days} å¤©")
    
    # æ£€æŸ¥ç‰¹å¾ä¸æ ‡ç­¾çš„ç›¸å…³æ€§
    print(f"\nğŸ”— ç‰¹å¾ä¸æ ‡ç­¾ç›¸å…³æ€§ (Top-10):")
    correlations = []
    for col in feature_cols:
        corr = abs(X[col].corr(y))
        correlations.append((col, corr))
    
    correlations.sort(key=lambda x: x[1], reverse=True)
    for i, (col, corr) in enumerate(correlations[:10]):
        print(f"{i+1:2d}. {col:20s}: {corr:.4f}")
    
    # æ£€æŸ¥å¯ç–‘ç‰¹å¾
    suspicious_features = []
    for col, corr in correlations:
        if corr > 0.1:  # ç›¸å…³æ€§è¶…è¿‡0.1çš„ç‰¹å¾
            suspicious_features.append((col, corr))
    
    print(f"\nâš ï¸ é«˜ç›¸å…³æ€§ç‰¹å¾ (|corr| > 0.1): {len(suspicious_features)} ä¸ª")
    for col, corr in suspicious_features[:10]:
        print(f"  {col}: {corr:.4f}")
    
    # æ£€æŸ¥æ”¶ç›Šç‡ç‰¹å¾
    ret_features = [col for col in feature_cols if 'ret_' in col]
    print(f"\nğŸ“ˆ æ”¶ç›Šç‡ç‰¹å¾åˆ†æ:")
    for col in ret_features:
        corr = abs(X[col].corr(y))
        print(f"  {col}: {corr:.4f}")
    
    # æ£€æŸ¥æ³¢åŠ¨ç‡ç‰¹å¾
    vol_features = [col for col in feature_cols if 'volatility' in col]
    print(f"\nğŸ“Š æ³¢åŠ¨ç‡ç‰¹å¾åˆ†æ:")
    for col in vol_features:
        corr = abs(X[col].corr(y))
        print(f"  {col}: {corr:.4f}")
    
    return suspicious_features

def test_feature_removal():
    """æµ‹è¯•ç§»é™¤å¯ç–‘ç‰¹å¾åçš„æ•ˆæœ"""
    print("\nğŸ§ª æµ‹è¯•ç‰¹å¾ç§»é™¤æ•ˆæœ...")
    
    # è¯»å–æ•°æ®
    df = pd.read_parquet('/Users/qiutianyu/data/processed/features_15m_fixed.parquet')
    
    # å‡†å¤‡ç‰¹å¾
    exclude_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'label', 'trend_1h', 'trend_4h']
    feature_cols = [col for col in df.columns if col not in exclude_cols and pd.api.types.is_numeric_dtype(df[col])]
    
    X = df[feature_cols].fillna(0)
    y = df['label']
    
    # æ—¶é—´æ’åº
    df_sorted = df.sort_values('timestamp').reset_index(drop=True)
    X_sorted = X.loc[df_sorted.index]
    y_sorted = y.loc[df_sorted.index]
    
    # åˆ†å‰²æ•°æ®
    split_idx = int(len(df_sorted) * 0.8)
    X_train = X_sorted[:split_idx]
    y_train = y_sorted[:split_idx]
    X_test = X_sorted[split_idx:]
    y_test = y_sorted[split_idx:]
    
    # æµ‹è¯•ä¸åŒç‰¹å¾ç»„åˆ
    test_cases = [
        ("å…¨éƒ¨ç‰¹å¾", feature_cols),
        ("ç§»é™¤æ”¶ç›Šç‡ç‰¹å¾", [col for col in feature_cols if 'ret_' not in col]),
        ("ç§»é™¤æ³¢åŠ¨ç‡ç‰¹å¾", [col for col in feature_cols if 'volatility' not in col]),
        ("åªä¿ç•™æŠ€æœ¯æŒ‡æ ‡", [col for col in feature_cols if any(x in col for x in ['rsi', 'macd', 'bb', 'adx', 'stoch', 'atr', 'ema'])]),
    ]
    
    results = []
    
    for case_name, selected_features in test_cases:
        print(f"\nğŸ”§ æµ‹è¯•: {case_name}")
        print(f"ç‰¹å¾æ•°é‡: {len(selected_features)}")
        
        X_train_subset = X_train[selected_features]
        X_test_subset = X_test[selected_features]
        
        # è®­ç»ƒæ¨¡å‹
        params = {
            'max_depth': 3,
            'n_estimators': 100,
            'learning_rate': 0.1,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'reg_alpha': 0.1,
            'reg_lambda': 1.0,
            'objective': 'binary:logistic',
            'eval_metric': 'auc',
            'use_label_encoder': False,
            'random_state': 42
        }
        
        model = xgb.XGBClassifier(**params)
        model.fit(X_train_subset, y_train, verbose=0)
        
        # è¯„ä¼°
        train_auc = roc_auc_score(y_train, model.predict_proba(X_train_subset)[:, 1])
        test_auc = roc_auc_score(y_test, model.predict_proba(X_test_subset)[:, 1])
        overfitting = train_auc - test_auc
        
        results.append({
            'case': case_name,
            'features': len(selected_features),
            'train_auc': train_auc,
            'test_auc': test_auc,
            'overfitting': overfitting
        })
        
        print(f"è®­ç»ƒAUC: {train_auc:.4f}")
        print(f"æµ‹è¯•AUC: {test_auc:.4f}")
        print(f"è¿‡æ‹Ÿåˆç¨‹åº¦: {overfitting:.4f}")
    
    # æ€»ç»“
    print(f"\nğŸ“Š ç‰¹å¾ç§»é™¤æµ‹è¯•æ€»ç»“:")
    for result in results:
        status = "âœ…" if result['overfitting'] < 0.05 else "âš ï¸"
        print(f"{status} {result['case']}: æµ‹è¯•AUC={result['test_auc']:.4f}, è¿‡æ‹Ÿåˆ={result['overfitting']:.4f}")
    
    return results

def main():
    print("ğŸ” æ•°æ®æ³„æ¼æ·±åº¦è¯Šæ–­")
    
    # è¯Šæ–­æ³„æ¼
    suspicious_features = diagnose_leakage()
    
    # æµ‹è¯•ç‰¹å¾ç§»é™¤
    results = test_feature_removal()
    
    print(f"\nğŸ¯ è¯Šæ–­ç»“è®º:")
    if suspicious_features:
        print(f"å‘ç° {len(suspicious_features)} ä¸ªé«˜ç›¸å…³æ€§ç‰¹å¾ï¼Œå¯èƒ½å­˜åœ¨æ•°æ®æ³„æ¼")
    else:
        print("æœªå‘ç°æ˜æ˜¾çš„æ•°æ®æ³„æ¼ç‰¹å¾")
    
    # æ‰¾å‡ºæœ€ä½³ç‰¹å¾ç»„åˆ
    best_result = min(results, key=lambda x: x['overfitting'])
    print(f"æœ€ä½³ç‰¹å¾ç»„åˆ: {best_result['case']}")
    print(f"æµ‹è¯•AUC: {best_result['test_auc']:.4f}")
    print(f"è¿‡æ‹Ÿåˆç¨‹åº¦: {best_result['overfitting']:.4f}")

if __name__ == "__main__":
    main() 